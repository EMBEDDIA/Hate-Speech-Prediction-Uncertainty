{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HS_Dataset_on_small_Cross_Valid_TF_W2V_ELMo_Universal_Encoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H79VEgAnrCY",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-qMpmv0n7zq",
        "colab_type": "code",
        "outputId": "76cdc942-7577-4097-cfb3-5f72a4c6dd53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "# Install packages\n",
        "!pip install xlrd\n",
        "!pip install gensim\n",
        "!pip install -U nltk\n",
        "!pip install spacy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.4)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.162)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.162 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.162)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.162->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.162->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/5d/825889810b85c303c8559a3fd74d451d80cf3585a851f2103e69576bf583/nltk-3.4.3.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 30.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/40/b7/c56ad418e6cd4d9e1e594b5e138d1ca6eec11a6ee3d464e5bb\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.3\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.9)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.12.0)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.28.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVE3b6K_nxck",
        "colab_type": "text"
      },
      "source": [
        "## Access to GDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qp3Er5joF3a",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwsBxiYLvRmi",
        "colab_type": "code",
        "outputId": "dc751478-d9c1-41c4-8b22-7954dfd80b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Load stop words and more (will be added later)\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBdZbRqeoLcr",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning dataset functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_lmXdnupUlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning data\n",
        "import unicodedata\n",
        "import re\n",
        "import gensim\n",
        "from nltk import tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "del stop_words[stop_words.index('not')]\n",
        "del stop_words[stop_words.index('your')]\n",
        "\n",
        "# Remove unwanted noise\n",
        "stop_words.append('rt')\n",
        "stop_words.append('wow')\n",
        "stop_words.append('ok')\n",
        "stop_words.append('mo')\n",
        "stop_words.append('dm')\n",
        "stop_words.append('idgaf')\n",
        "\n",
        "CONTRACTION_MAP = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"hehe will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"hey would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"tthey will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\",\n",
        "\"fvcking\": \"fucking\",\n",
        "\"fking\": \"fucking\",\n",
        "# Correction\n",
        "    # \"ill\": \"I will\",\n",
        "    \"seriois\": \"serious\",\n",
        "    #\"mo\": \"my\",\n",
        "    #\"ass\": \"asss\",\n",
        "    \"lmaoooooooo\": \"lmao\",\n",
        "    \"uncomf\": \"uncomfortable\",\n",
        "    \"pls\": \"please\",\n",
        "    \"lowlife\": \"low life\",\n",
        "    \"puss\": \"pussy\",\n",
        "    # Hashtash expand\n",
        "    \"ihatefemales\": \"I hate females\",\n",
        "    \"yesallwomen\": \"yes all women\",\n",
        "    \"sendthemhome\": \"send them home\",\n",
        "    \"stoptheinvasion\": \"stop the invation\",\n",
        "    \"buildthewall\": \"build the wall\",\n",
        "    \"womensuck\": \"women suck\",\n",
        "    \"stopimmigration\": \"stop immigration\",\n",
        "    \"sendthemback\": \"send them back\"\n",
        "}\n",
        "\n",
        "# Expand the contractions\n",
        "def expand_contractions(s, contractions_dict=CONTRACTION_MAP):\n",
        "    contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, s)\n",
        "\n",
        "# Sentences to words\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "        \n",
        "# tokenize\n",
        "def sent_tokenize(sents):\n",
        "    return tokenize.sent_tokenize(sents)\n",
        "  \n",
        "# Function to remove some unwanted info\n",
        "def remove_info(data, remove_list):\n",
        "    for i, sent in enumerate(data):\n",
        "        for com in remove_list:\n",
        "            if com.strip() != '':\n",
        "                sent = sent.lower().replace(com.lower(), '')\n",
        "        data[i] = sent\n",
        "    return data\n",
        "\n",
        "# Remove special characters; only get ASCII\n",
        "def remove_accents(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', str(input_str))\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    only_ascii = str(only_ascii)[2:-1]\n",
        "    return only_ascii\n",
        "\n",
        "# Remove emails, link, tweeter account, ..\n",
        "def clean(texts):\n",
        "    # Remove Emails\n",
        "    data = [re.sub('\\S*@\\S*\\s?', '', sent.lower().strip()) for sent in texts]\n",
        "    # Remove @username\n",
        "    data = [re.sub('@\\S*\\s?', '', sent) for sent in data]\n",
        "    # Remove link\n",
        "    data = [re.sub(r'http\\S+', '', sent) for sent in data]\n",
        "    # Remove new line characters\n",
        "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "    # Remove multi dots\n",
        "    data = [re.sub('\\.\\.', '', sent) for sent in data]\n",
        "    # Remove distracting quotes\n",
        "    # data = [re.sub(\"\\\"\", \"\", sent) for sent in data]\n",
        "    # Remove #\n",
        "    data = [re.sub(\"#\", \"\", sent) for sent in data]\n",
        "    # Remove number\n",
        "    data = [re.sub(r\"\\d+\", '', sent) for sent in data]\n",
        "    return data\n",
        "\n",
        "# Func to remove stop words\n",
        "def remove_stopwords(texts, stop_words=stop_words):\n",
        "    return [' '.join([word.replace(' ', '') \n",
        "                for word in gensim.utils.simple_preprocess(str(doc)) \\\n",
        "                if word.replace(' ', '') not in stop_words]) for doc in texts]\n",
        "\n",
        "# Func to lemmatize words using spacy\n",
        "def lemmatization(texts, stop_words=None, \n",
        "                  allowed_postags=('NOUN', 'VERB', 'ADV')):\n",
        "    nlp = en_core_web_md.load()\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc \n",
        "                            if token.pos_ in allowed_postags \\\n",
        "                              and token.lemma_ not in stop_words])\n",
        "    return texts_out\n",
        "\n",
        "# Func to lemmatize words using nltk\n",
        "# Refer to https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
        "def lemmatization_nltk(sents):\n",
        "    # input: sents -> list of sentences\n",
        "    # output: list of lemmatized words\n",
        "    def get_wordnet_pos(word):\n",
        "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\n",
        "                    \"N\": wordnet.NOUN,\n",
        "                    \"V\": wordnet.VERB,\n",
        "                    \"R\": wordnet.ADV}\n",
        "\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "    lemmatized_words = []\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for sentence in sents:\n",
        "        lemmatized_words.append(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) \n",
        "                                  for w in nltk.word_tokenize(sentence)]))\n",
        "    return lemmatized_words\n",
        "\n",
        "# Normalized pipeline\n",
        "def normalization_pipeline(texts):\n",
        "    normalized_texts = []\n",
        "    # lower text\n",
        "    normalized_texts = [text.lower() for text in texts]\n",
        "    \n",
        "    # Clean text: special words, numbers, ..\n",
        "    normalized_texts = clean(normalized_texts)\n",
        "    # Expand contraction \n",
        "    normalized_texts = [expand_contractions(text) for text in normalized_texts]\n",
        "    normailzed_texts = [remove_accents(text) for text in normalized_texts]\n",
        "    # Break paragraph into sentents\n",
        "    normalized_texts = [sent_tokenize(text) for text in normalized_texts]\n",
        "    \n",
        "    # Remove stopwords\n",
        "    normalized_texts = remove_stopwords(normalized_texts)\n",
        "    \n",
        "    normalized_texts = lemmatization_nltk(normalized_texts)\n",
        "    return normalized_texts\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCgC7BZPE_ss",
        "colab_type": "code",
        "outputId": "a3a2fcc5-c3c9-4a87-b8c2-257b854f3067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Read data from file\n",
        "import pandas as pd\n",
        "\n",
        "# Point to the file in Google Drive\n",
        "filename='/content/gdrive/My Drive/trial_en.tsv'\n",
        "df = pd.read_csv(filename, sep='\\t')\n",
        "corpus = df['text'].values\n",
        "\n",
        "normalized_texts = normalization_pipeline(corpus)\n",
        "\n",
        "df['normalized_text'] = normalized_texts\n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id  ...                                    normalized_text\n",
            "0   1  ...                           shut fuck come suck dick\n",
            "1   2  ...  fuck say leave block first gon na kick your as...\n",
            "2   3  ...  cock get hard want pull your panty push door l...\n",
            "3   4  ...  ill kill bitch chloe your not home kid bitch r...\n",
            "4   5  ...         get rape beautiful woman like work project\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy3_6NkI77gV",
        "colab_type": "code",
        "outputId": "8c752235-9548-4d74-f61b-e39b1894b852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=2,\n",
        "                           norm='l2',\n",
        "                           smooth_idf=True,\n",
        "                           use_idf=True,\n",
        "                           ngram_range=(1, 1))\n",
        "features = vectorizer.fit_transform(corpus)\n",
        "print(features.todense())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.26331868 0.18521249]\n",
            " [0.         0.         0.         ... 0.27264559 0.23875348 0.33586775]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY-ejG4bohsh",
        "colab_type": "text"
      },
      "source": [
        "# Feature enginerring\n",
        "Bow, TF-IDF and Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DgO4GhYMBqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature extraction\n",
        "# Use TF and TF-IDF from scikit-learn\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import pandas as pd\n",
        "\n",
        "def bow_extractor(corpus, ngram_range=(2,2)):\n",
        "    vectorizer = CountVectorizer(min_df=2, ngram_range=ngram_range)\n",
        "    features = vectorizer.fit_transform(corpus)\n",
        "    return vectorizer, features\n",
        "\n",
        "def tfidf_extractor(corpus, ngram_range=(2,2)):\n",
        "    vectorizer = TfidfVectorizer(min_df=2,\n",
        "                               norm='l2',\n",
        "                               smooth_idf=True,\n",
        "                               use_idf=True,\n",
        "                               ngram_range=ngram_range)\n",
        "    features = vectorizer.fit_transform(corpus)\n",
        "    return vectorizer, features\n",
        "  \n",
        "def tfidf_transformer(bow_matrix):\n",
        "    transformer = TfidfTransformer(norm='l2',\n",
        "                                 smooth_idf=True,\n",
        "                                 use_idf=True)\n",
        "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
        "    return transformer, tfidf_matrix\n",
        "\n",
        "def display_features(features, feature_names):\n",
        "    df = pd.DataFrame(data=features,\n",
        "                    columns=feature_names)\n",
        "    print(df)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWUQJvyGuYlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Demo Feature extraction with count vector and tf-idf\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "train_data_features = vectorizer.fit_transform(df['normalized_text'].values.copy())\n",
        "vocab = vectorizer.get_feature_names()\n",
        "\n",
        "# Sum up the counts of each vocabulary word\n",
        "dist = np.sum(train_data_features.toarray(), axis=0)\n",
        "\n",
        "# Print Word Count\n",
        "# for tag, count in zip(vocab, dist):\n",
        "#    print(count, tag)\n",
        "\n",
        "corpus = df['normalized_text'].values.copy()\n",
        "bow_vect, bow_features = bow_extractor(corpus)\n",
        "features = bow_features.todense()\n",
        "feature_names = bow_vect.get_feature_names()\n",
        "# display_features(features, feature_names)\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer, tdidf_features = tfidf_extractor(corpus.copy())\n",
        "features = tdidf_features.todense()\n",
        "feature_names = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "#display_features(np.round(tdidf_features.todense(), 2), feature_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfC0SqUUR2T8",
        "colab_type": "code",
        "outputId": "aaf30d06-4ca5-4539-8591-1ffa738e407b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# word2vec feature extraction\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "# define function to average word vectors for a text document    \n",
        "def average_word_vectors(words, model, vocabulary, num_features):\n",
        "    feature_vector = np.zeros((num_features,),dtype=\"float32\")\n",
        "    nwords = 0.\n",
        "    for word in words:\n",
        "        if word in vocabulary:\n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model[word])\n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "    return feature_vector\n",
        "\n",
        "# generalize above function for a corpus of documents  \n",
        "def averaged_word_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    features = [average_word_vectors(tokenized_sentence, model, \n",
        "                                   vocabulary, num_features)\n",
        "                  for tokenized_sentence in corpus]\n",
        "    return np.array(features)\n",
        "\n",
        "# define function to compute tfidf weighted averaged word vector for a document\n",
        "def tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, num_features):\n",
        "    word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)] \n",
        "                 if tfidf_vocabulary.get(word) else 0 for word in words]\n",
        "    word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}\n",
        "    feature_vector = np.zeros((num_features,),dtype=\"float32\")\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    wts = 0.\n",
        "    for word in words:\n",
        "        if word in vocabulary:\n",
        "            word_vector = model[word]\n",
        "            weighted_word_vector = word_tfidf_map[word] * word_vector\n",
        "            wts = wts + word_tfidf_map[word]\n",
        "        feature_vector = np.add(feature_vector, weighted_word_vector)\n",
        "    if wts:\n",
        "        feature_vector = np.divide(feature_vector, wts)\n",
        "    return feature_vector\n",
        "\n",
        "# generalize above function for a corpus of documents    \n",
        "def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors, tfidf_vocabulary, model, num_features):\n",
        "    docs_tfidfs = [(doc, doc_tfidf)\n",
        "                 for doc, doc_tfidf\n",
        "                 in zip(corpus, tfidf_vectors)]\n",
        "    features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary,\n",
        "                                 model, num_features)\n",
        "                  for tokenized_sentence, tfidf in docs_tfidfs]\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "corpus = df['normalized_text'].values.copy()\n",
        "tokenized_corpus = [nltk.word_tokenize(sent) for sent in corpus]\n",
        "\n",
        "\n",
        "model = Word2Vec(tokenized_corpus, min_count=2, workers=4)\n",
        "avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_corpus,\n",
        "                                                 model=model,\n",
        "                                                 num_features=100)\n",
        "\n",
        "print(np.round(avg_word_vec_features, 5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0608 11:31:23.912069 140007620867968 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[-1.92e-03  7.80e-04  2.00e-04 ... -1.05e-03  2.90e-04 -7.30e-04]\n",
            " [ 9.00e-05  1.11e-03 -1.55e-03 ... -9.40e-04 -1.07e-03 -8.00e-04]\n",
            " [ 4.60e-04  8.50e-04 -2.00e-03 ... -8.00e-05  1.10e-03 -9.80e-04]\n",
            " ...\n",
            " [-2.40e-04 -3.40e-04  7.80e-04 ...  1.47e-03 -8.60e-04 -7.10e-04]\n",
            " [-4.34e-03  1.08e-03 -2.20e-04 ... -2.68e-03 -1.55e-03 -2.19e-03]\n",
            " [ 0.00e+00  0.00e+00  0.00e+00 ...  0.00e+00  0.00e+00  0.00e+00]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2v6jgoLouEp",
        "colab_type": "text"
      },
      "source": [
        "## Preparing Data before using models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwLcDdtvSW1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare data for modeling\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n",
        "    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels,\n",
        "                                                      test_size=test_data_proportion, \n",
        "                                                      random_state=42,\n",
        "                                                      shuffle=True)\n",
        "    return train_X, test_X, train_Y, test_Y\n",
        "\n",
        "def get_metrics(true_labels, predicted_labels):\n",
        "    print('Accuracy:', \n",
        "        np.round(metrics.accuracy_score(true_labels, predicted_labels), 2))\n",
        "    print('Precision:', \n",
        "        np.round(metrics.precision_score(true_labels,\n",
        "                                         predicted_labels,\n",
        "                                         average='weighted'),2))\n",
        "    print('Recall:', \n",
        "        np.round(metrics.recall_score(true_labels,\n",
        "                                      predicted_labels,\n",
        "                                      average='weighted'),2))\n",
        "    print('F1 Score:', \n",
        "        np.round(metrics.f1_score(true_labels,\n",
        "                                  predicted_labels,\n",
        "                                  average='weighted'),2))\n",
        "    return\n",
        "\n",
        "def train_predict_evaluate_model(classifier, train_features, \n",
        "                                 train_labels, test_features, test_labels):\n",
        "    classifier.fit(train_features, train_labels)\n",
        "    predictions = classifier.predict(test_features)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GQ6nmUgu7Xk",
        "colab_type": "text"
      },
      "source": [
        "## Defined Accuracy Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOHQOPmPeDAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def accuracy_report(yvalid, prediction):\n",
        "    cm1 = confusion_matrix(yvalid, prediction)\n",
        "    print('Confusion Matrix :', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "\n",
        "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "\n",
        "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "    \n",
        "    print('                 ')\n",
        "    print('Accuracy:', accuracy_score(yvalid, prediction))\n",
        "    print('Precision:', precision_score(yvalid, prediction))\n",
        "    print('Recall:', recall_score(yvalid, prediction))\n",
        "    print('F1:', f1_score(yvalid, prediction))\n",
        "    return accuracy_score(yvalid, prediction), f1_score(yvalid, prediction), \\\n",
        "            recall_score(yvalid, prediction), precision_score(yvalid, prediction)\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKcqsmE-pAto",
        "colab_type": "text"
      },
      "source": [
        "# Cross-validation with TFIDF and Word2Vec\n",
        "\n",
        "Support vector machine, Logistic Regression with TFIDF and Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USzKFjglelQI",
        "colab_type": "text"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGCIBfb8s1W4",
        "colab_type": "text"
      },
      "source": [
        "### SVM Cross validation with Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R0LMv1gtpvtq",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "np.random.seed(1)\n",
        "\n",
        "params = {\n",
        "        'C': [0.01, 0.1, 1., 10, 50, 100],\n",
        "        'kernel': ['rbf'],\n",
        "        'gamma': [0.01, 0.1, 1., 10., 100.]\n",
        "}\n",
        "\n",
        "corpus = df['normalized_text'].values\n",
        "labels = df['HS'].values\n",
        "\n",
        "X = corpus.copy()\n",
        "y = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWKZD0s3S-wy",
        "colab_type": "text"
      },
      "source": [
        "#### New results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLkdI9UFPuCX",
        "colab_type": "code",
        "outputId": "70587482-d6f4-4511-a0fa-ae71aa301725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "np.random.seed(1)\n",
        "\n",
        "corpus = df['normalized_text'].values\n",
        "labels = df['HS'].values\n",
        "\n",
        "X = [nltk.word_tokenize(text) for text in corpus]\n",
        "X = np.array(X)\n",
        "y = labels\n",
        "# averaged word vector features\n",
        "kf = KFold(n_splits=5, random_state=42, \n",
        "           shuffle=True)\n",
        "\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "\n",
        "w2vmodel = gensim.models.Word2Vec(X,\n",
        "                               size=500, \n",
        "                               min_count=2,\n",
        "                               window=1, \n",
        "                               workers=4)\n",
        "best_params = None\n",
        "\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    avg_wv_train_features = averaged_word_vectorizer(corpus=x_train, \n",
        "                                                     model=w2vmodel, \n",
        "                                                     num_features=500)\n",
        "    avg_wv_test_features = averaged_word_vectorizer(corpus=x_test, \n",
        "                                                    model=w2vmodel, \n",
        "                                                    num_features=500)\n",
        "    if best_params is None:\n",
        "        svm = SVC(random_state=42)\n",
        "        random_search = RandomizedSearchCV(svm, \n",
        "                                           param_distributions=params, \n",
        "                                           cv=4, \n",
        "                                           random_state=42)\n",
        "        random_search.fit(avg_wv_train_features, y_train)\n",
        "        best_params = random_search.best_params_\n",
        "        print(best_params)\n",
        "    \n",
        "    svm = SVC(random_state=42, C=best_params['C'], \n",
        "             kernel=best_params['kernel'],\n",
        "             gamma=best_params['gamma'])\n",
        "    print('SVM results with AVG word vector')\n",
        "    svm_predictions = train_predict_evaluate_model(classifier=svm, \n",
        "                                                 train_features=avg_wv_train_features, \n",
        "                                                 train_labels=y_train,\n",
        "                                                 test_features=avg_wv_test_features, \n",
        "                                                 test_labels=y_test)\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, svm_predictions)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    \n",
        "    \n",
        "print('SVM Overall with Word2Vec')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0608 11:32:19.446745 140007620867968 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'kernel': 'rbf', 'gamma': 100.0, 'C': 50}\n",
            "SVM results with AVG word vector\n",
            "Confusion Matrix : [[4 7]\n",
            " [1 8]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.5333333333333333\n",
            "Recall: 0.8888888888888888\n",
            "F1: 0.6666666666666667\n",
            "SVM results with AVG word vector\n",
            "Confusion Matrix : [[5 5]\n",
            " [2 8]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6153846153846154\n",
            "Recall: 0.8\n",
            "F1: 0.6956521739130435\n",
            "SVM results with AVG word vector\n",
            "Confusion Matrix : [[5 5]\n",
            " [1 9]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.6428571428571429\n",
            "Recall: 0.9\n",
            "F1: 0.75\n",
            "SVM results with AVG word vector\n",
            "Confusion Matrix : [[4 7]\n",
            " [0 9]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.5625\n",
            "Recall: 1.0\n",
            "F1: 0.72\n",
            "SVM results with AVG word vector\n",
            "Confusion Matrix : [[5 3]\n",
            " [5 7]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.7\n",
            "Recall: 0.5833333333333334\n",
            "F1: 0.6363636363636365\n",
            "SVM Overall with Word2Vec\n",
            "Acc= 0.64 , std= 0.03741657386773941\n",
            "f1 score= 0.6937364953886693 , std= 0.03970754092001219\n",
            "Precision= 0.6108150183150183 , std= 0.0588885259164867\n",
            "Recalls= 0.8344444444444445 , std= 0.14065104354174807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h6l6w-aTe63",
        "colab_type": "text"
      },
      "source": [
        "#### Old results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ixlq4lMpaMQ",
        "colab_type": "code",
        "outputId": "6477a1e2-6a9e-4ee6-ffa9-f571dd58a19b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1057
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "np.random.seed(1)\n",
        "\n",
        "corpus = df['normalized_text'].values\n",
        "labels = df['HS'].values\n",
        "\n",
        "X = [nltk.word_tokenize(text) for text in corpus]\n",
        "X = np.array(X)\n",
        "y = labels\n",
        "# averaged word vector features\n",
        "kf = KFold(n_splits=5, random_state=42, \n",
        "           shuffle=True)\n",
        "\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "\n",
        "w2vmodel = gensim.models.Word2Vec(X,\n",
        "                               size=500, \n",
        "                               min_count=2,\n",
        "                               window=1, \n",
        "                               workers=4)\n",
        "best_params = None\n",
        "\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    avg_wv_train_features = averaged_word_vectorizer(corpus=x_train, \n",
        "                                                     model=w2vmodel, \n",
        "                                                     num_features=500)\n",
        "    avg_wv_test_features = averaged_word_vectorizer(corpus=x_test, \n",
        "                                                    model=w2vmodel, \n",
        "                                                    num_features=500)\n",
        "    svm = SVC(random_state=42)\n",
        "    random_search = RandomizedSearchCV(svm, \n",
        "                                       param_distributions=params, \n",
        "                                       cv=4, \n",
        "                                       random_state=42)\n",
        "    random_search.fit(avg_wv_train_features, y_train)\n",
        "    best_params = random_search.best_params_\n",
        "    svm = SVC(random_state=42, C=best_params['C'], \n",
        "             kernel=best_params['kernel'],\n",
        "             gamma=best_params['gamma'])\n",
        "    print('SVM results with AVG word vector')\n",
        "    svm_predictions = train_predict_evaluate_model(classifier=svm, \n",
        "                                                 train_features=avg_wv_train_features, \n",
        "                                                 train_labels=y_train,\n",
        "                                                 test_features=avg_wv_test_features, \n",
        "                                                 test_labels=y_test)\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, svm_predictions)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    \n",
        "    \n",
        "print('SVM Overall with Word2Vec')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0608 11:32:28.522340 140007620867968 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SVM results with AVG word vector\n",
            "Confusion Matrix : [[4 7]\n",
            " [1 8]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.5333333333333333\n",
            "Recall: 0.8888888888888888\n",
            "F1: 0.6666666666666667\n",
            "SVM results with AVG word vector\n",
            "Confusion Matrix : [[5 5]\n",
            " [2 8]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6153846153846154\n",
            "Recall: 0.8\n",
            "F1: 0.6956521739130435\n",
            "SVM results with AVG word vector\n",
            "Confusion Matrix : [[5 5]\n",
            " [1 9]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.6428571428571429\n",
            "Recall: 0.9\n",
            "F1: 0.75\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SVM results with AVG word vector\n",
            "Confusion Matrix : [[4 7]\n",
            " [0 9]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.5625\n",
            "Recall: 1.0\n",
            "F1: 0.72\n",
            "SVM results with AVG word vector\n",
            "Confusion Matrix : [[5 3]\n",
            " [5 7]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.7\n",
            "Recall: 0.5833333333333334\n",
            "F1: 0.6363636363636365\n",
            "SVM Overall with Word2Vec\n",
            "Acc= 0.64 , std= 0.03741657386773941\n",
            "f1 score= 0.6937364953886693 , std= 0.03970754092001219\n",
            "Precision= 0.6108150183150183 , std= 0.0588885259164867\n",
            "Recalls= 0.8344444444444445 , std= 0.14065104354174807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VcH38SGtwxe",
        "colab_type": "text"
      },
      "source": [
        "### SVM cross validation with TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMyjxWq65nz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "np.random.seed(1)\n",
        "params = {\n",
        "        'C': [0.01, 0.1, 1., 10, 50, 100],\n",
        "        'kernel': ['rbf'],\n",
        "        'gamma': [0.01, 0.1, 1., 10., 100.]\n",
        "}\n",
        "\n",
        "corpus = df['normalized_text'].values\n",
        "labels = df['HS'].values\n",
        "\n",
        "X = corpus.copy()\n",
        "y = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trcRjwVnTmcS",
        "colab_type": "text"
      },
      "source": [
        "#### New results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svuik3qpQN9D",
        "colab_type": "code",
        "outputId": "9b0f91af-6ecb-48a1-a880-c6979e58eeda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "best_params = None\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
        "                                   max_features=300)\n",
        "    \n",
        "    tfidf_train_features = tfidf_vectorizer.fit_transform(x_train)\n",
        "    tfidf_test_features = tfidf_vectorizer.transform(x_test)\n",
        "    if best_params is None:\n",
        "        svm = SVC()\n",
        "        random_search = RandomizedSearchCV(svm, param_distributions=params, \n",
        "                                           cv=4, random_state=42)\n",
        "        random_search.fit(tfidf_train_features, y_train)\n",
        "        best_params = random_search.best_params_\n",
        "    \n",
        "    svm = SVC(random_state=42, C=best_params['C'], \n",
        "             kernel=best_params['kernel'],\n",
        "             gamma=best_params['gamma'])\n",
        "    predictions = train_predict_evaluate_model(classifier=svm,\n",
        "                                               train_features=tfidf_train_features, \n",
        "                                               train_labels=y_train,\n",
        "                                               test_features=tfidf_test_features, \n",
        "                                               test_labels=y_test)\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    \n",
        "    \n",
        "print('SVM Overall with TFIDF')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix : [[6 5]\n",
            " [3 6]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.5454545454545454\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.6\n",
            "Confusion Matrix : [[7 3]\n",
            " [4 6]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.6\n",
            "F1: 0.631578947368421\n",
            "Confusion Matrix : [[6 4]\n",
            " [4 6]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.6\n",
            "Recall: 0.6\n",
            "F1: 0.6\n",
            "Confusion Matrix : [[5 6]\n",
            " [3 6]]\n",
            "                 \n",
            "Accuracy: 0.55\n",
            "Precision: 0.5\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.5714285714285715\n",
            "Confusion Matrix : [[6 2]\n",
            " [3 9]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.8181818181818182\n",
            "Recall: 0.75\n",
            "F1: 0.7826086956521738\n",
            "SVM Overall with TFIDF\n",
            "Acc= 0.6300000000000001 , std= 0.06782329983125268\n",
            "f1 score= 0.6371232428898332 , std= 0.07519151064051076\n",
            "Precision= 0.6260606060606061 , std= 0.11101998099433111\n",
            "Recalls= 0.6566666666666666 , std= 0.05537749241945384\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU3OT-uOTo69",
        "colab_type": "text"
      },
      "source": [
        "#### Old results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRnGZz87t1ey",
        "colab_type": "code",
        "outputId": "90be9ff9-dd23-4946-b196-4abe8bc2f950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        }
      },
      "source": [
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "best_params = None\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
        "                                   max_features=300)\n",
        "    \n",
        "    tfidf_train_features = tfidf_vectorizer.fit_transform(x_train)\n",
        "    tfidf_test_features = tfidf_vectorizer.transform(x_test)\n",
        "    \n",
        "    svm = SVC()\n",
        "    random_search = RandomizedSearchCV(svm, param_distributions=params, \n",
        "                                       cv=4, random_state=42)\n",
        "    random_search.fit(tfidf_train_features, y_train)\n",
        "    best_params = random_search.best_params_\n",
        "    \n",
        "    svm = SVC(random_state=42, C=best_params['C'], \n",
        "             kernel=best_params['kernel'],\n",
        "             gamma=best_params['gamma'])\n",
        "    predictions = train_predict_evaluate_model(classifier=svm,\n",
        "                                               train_features=tfidf_train_features, \n",
        "                                               train_labels=y_train,\n",
        "                                               test_features=tfidf_test_features, \n",
        "                                               test_labels=y_test)\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    \n",
        "    \n",
        "print('SVM Overall with TFIDF')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix : [[6 5]\n",
            " [3 6]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.5454545454545454\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.6\n",
            "Confusion Matrix : [[7 3]\n",
            " [4 6]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.6\n",
            "F1: 0.631578947368421\n",
            "Confusion Matrix : [[6 4]\n",
            " [3 7]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6363636363636364\n",
            "Recall: 0.7\n",
            "F1: 0.6666666666666666\n",
            "Confusion Matrix : [[5 6]\n",
            " [3 6]]\n",
            "                 \n",
            "Accuracy: 0.55\n",
            "Precision: 0.5\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.5714285714285715\n",
            "Confusion Matrix : [[5 3]\n",
            " [3 9]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.75\n",
            "Recall: 0.75\n",
            "F1: 0.75\n",
            "SVM Overall with TFIDF\n",
            "Acc= 0.6300000000000001 , std= 0.05099019513592783\n",
            "f1 score= 0.6439348370927318 , std= 0.061816909240319934\n",
            "Precision= 0.6196969696969696 , std= 0.08865902326259269\n",
            "Recalls= 0.6766666666666666 , std= 0.04898979485566357\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efker9Ghetwr",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjjJ1xSQ4lAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "params = {\n",
        "        'C': [0.01, 0.1, 1., 10, 20, 100],\n",
        "        'solver': ['liblinear', 'lbfgs'],\n",
        "        'penalty': ['l2']\n",
        "}\n",
        "\n",
        "corpus = df['normalized_text'].values\n",
        "labels = df['HS'].values\n",
        "\n",
        "X = [nltk.word_tokenize(text) for text in corpus]\n",
        "X = np.array(X)\n",
        "y = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ-2b_-ctFNX",
        "colab_type": "text"
      },
      "source": [
        "### Losgistic Regression cross validation with Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyDwlBkZTZ03",
        "colab_type": "text"
      },
      "source": [
        "#### New results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh7oBIVIQhwF",
        "colab_type": "code",
        "outputId": "86d53d59-d466-4a72-b657-deb53eb80fbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1125
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "\n",
        "model = gensim.models.Word2Vec(X,\n",
        "                               size=500, \n",
        "                               min_count=2,\n",
        "                               window=1, \n",
        "                               workers=4)\n",
        "best_params = None\n",
        "for train_index, test_index in kf.split(X):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    avg_wv_train_features = averaged_word_vectorizer(corpus=x_train, \n",
        "                                                     model=model, \n",
        "                                                     num_features=500)\n",
        "    avg_wv_test_features = averaged_word_vectorizer(corpus=x_test, \n",
        "                                                    model=model, \n",
        "                                                    num_features=500)\n",
        "    if best_params is None:\n",
        "        logistic = LogisticRegression(random_state=42)\n",
        "        random_search = RandomizedSearchCV(logistic, random_state=42,\n",
        "                                           param_distributions=params, cv=4)\n",
        "        random_search.fit(avg_wv_train_features, y_train)\n",
        "        best_params = random_search.best_params_\n",
        "        \n",
        "    logistic = LogisticRegression(solver=best_params['solver'],\n",
        "                                 C=best_params['C'], penalty='l2',\n",
        "                                 random_state=42)\n",
        "    # Averaged word vector\n",
        "    print('Logistic Regression results with AVG word vector')\n",
        "    # Support Vector Machine with averaged word vector features use word2vec\n",
        "    predictions = train_predict_evaluate_model(classifier=logistic, \n",
        "                                               train_features=avg_wv_train_features, \n",
        "                                               train_labels=y_train,\n",
        "                                               test_features=avg_wv_test_features, \n",
        "                                               test_labels=y_test)\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    \n",
        "    \n",
        "print('Logistic Overall with Word2Vec')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0608 11:35:43.863175 140007620867968 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression results with AVG word vector\n",
            "Confusion Matrix : [[ 0 11]\n",
            " [ 0  9]]\n",
            "                 \n",
            "Accuracy: 0.45\n",
            "Precision: 0.45\n",
            "Recall: 1.0\n",
            "F1: 0.6206896551724138\n",
            "Logistic Regression results with AVG word vector\n",
            "Confusion Matrix : [[7 3]\n",
            " [3 7]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.7\n",
            "Recall: 0.7\n",
            "F1: 0.7\n",
            "Logistic Regression results with AVG word vector\n",
            "Confusion Matrix : [[6 4]\n",
            " [1 9]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.6923076923076923\n",
            "Recall: 0.9\n",
            "F1: 0.7826086956521738\n",
            "Logistic Regression results with AVG word vector\n",
            "Confusion Matrix : [[ 0 11]\n",
            " [ 0  9]]\n",
            "                 \n",
            "Accuracy: 0.45\n",
            "Precision: 0.45\n",
            "Recall: 1.0\n",
            "F1: 0.6206896551724138\n",
            "Logistic Regression results with AVG word vector\n",
            "Confusion Matrix : [[ 8  0]\n",
            " [12  0]]\n",
            "                 \n",
            "Accuracy: 0.4\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1: 0.0\n",
            "Logistic Overall with Word2Vec\n",
            "Acc= 0.55 , std= 0.14491376746189435\n",
            "f1 score= 0.5447976011994002 , std= 0.2789141571248734\n",
            "Precision= 0.4584615384615384 , std= 0.25430494670210363\n",
            "Recalls= 0.72 , std= 0.3762977544445356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N6z87EOUBDY",
        "colab_type": "text"
      },
      "source": [
        "#### Old results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEQ7gKAlo5eN",
        "colab_type": "code",
        "outputId": "e2b7216a-fb1c-4be6-9b2e-9cfffbbce1ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1065
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "\n",
        "model = gensim.models.Word2Vec(X,\n",
        "                               size=500, \n",
        "                               min_count=2,\n",
        "                               window=1, \n",
        "                               workers=4)\n",
        "for train_index, test_index in kf.split(X):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    avg_wv_train_features = averaged_word_vectorizer(corpus=x_train, \n",
        "                                                     model=model, \n",
        "                                                     num_features=500)\n",
        "    avg_wv_test_features = averaged_word_vectorizer(corpus=x_test, \n",
        "                                                    model=model, \n",
        "                                                    num_features=500)\n",
        "    \n",
        "    logistic = LogisticRegression(random_state=42)\n",
        "    random_search = RandomizedSearchCV(logistic, random_state=42,\n",
        "                                       param_distributions=params, cv=4)\n",
        "    random_search.fit(avg_wv_train_features, y_train)\n",
        "    best_params = random_search.best_params_\n",
        "        \n",
        "    logistic = LogisticRegression(solver=best_params['solver'],\n",
        "                                 C=best_params['C'], penalty='l2',\n",
        "                                 random_state=42)\n",
        "    # Averaged word vector\n",
        "    print('Logistic Regression results with AVG word vector')\n",
        "    # Support Vector Machine with averaged word vector features use word2vec\n",
        "    predictions = train_predict_evaluate_model(classifier=logistic, \n",
        "                                               train_features=avg_wv_train_features, \n",
        "                                               train_labels=y_train,\n",
        "                                               test_features=avg_wv_test_features, \n",
        "                                               test_labels=y_test)\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    \n",
        "    \n",
        "print('Logistic Overall with Word2Vec')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression results with AVG word vector\n",
            "Confusion Matrix : [[ 0 11]\n",
            " [ 0  9]]\n",
            "                 \n",
            "Accuracy: 0.45\n",
            "Precision: 0.45\n",
            "Recall: 1.0\n",
            "F1: 0.6206896551724138\n",
            "Logistic Regression results with AVG word vector\n",
            "Confusion Matrix : [[8 2]\n",
            " [3 7]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.7777777777777778\n",
            "Recall: 0.7\n",
            "F1: 0.7368421052631577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression results with AVG word vector\n",
            "Confusion Matrix : [[4 6]\n",
            " [1 9]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6\n",
            "Recall: 0.9\n",
            "F1: 0.7200000000000001\n",
            "Logistic Regression results with AVG word vector\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix : [[ 0 11]\n",
            " [ 0  9]]\n",
            "                 \n",
            "Accuracy: 0.45\n",
            "Precision: 0.45\n",
            "Recall: 1.0\n",
            "F1: 0.6206896551724138\n",
            "Logistic Regression results with AVG word vector\n",
            "Confusion Matrix : [[ 8  0]\n",
            " [12  0]]\n",
            "                 \n",
            "Accuracy: 0.4\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1: 0.0\n",
            "Logistic Overall with Word2Vec\n",
            "Acc= 0.54 , std= 0.13564659966250536\n",
            "f1 score= 0.5396442831215971 , std= 0.27414150456858233\n",
            "Precision= 0.4555555555555556 , std= 0.257792145193481\n",
            "Recalls= 0.72 , std= 0.3762977544445356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3QSdDQntjrH",
        "colab_type": "text"
      },
      "source": [
        "### Logistic cross validation with TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oJ4tau13bTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "np.random.seed(1)\n",
        "params = {\n",
        "        'C': [0.01, 0.1, 1., 10, 20, 100],\n",
        "        'solver': ['liblinear', 'lbfgs'],\n",
        "        'penalty': ['l2']\n",
        "}\n",
        "corpus = df['normalized_text'].values\n",
        "labels = df['HS'].values\n",
        "\n",
        "X = corpus.copy()\n",
        "y = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPBdOPkxUMTR",
        "colab_type": "text"
      },
      "source": [
        "#### New results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE8vOe1RQzVi",
        "colab_type": "code",
        "outputId": "e8729fc5-b42f-4d1b-aaed-3a8e5d14f1e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "\n",
        "best_params = None\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
        "                                   max_features=300)\n",
        "    \n",
        "    tfidf_train_features = tfidf_vectorizer.fit_transform(x_train)\n",
        "    tfidf_test_features = tfidf_vectorizer.transform(x_test)\n",
        "    \n",
        "    if best_params is None:\n",
        "        logistic = LogisticRegression(random_state=42)\n",
        "        random_search = RandomizedSearchCV(logistic, param_distributions=params, \n",
        "                                           cv=4, random_state=42)\n",
        "        random_search.fit(tfidf_train_features, y_train)\n",
        "        best_params = random_search.best_params_\n",
        "        print(best_params)\n",
        "    \n",
        "    logistic = LogisticRegression(solver=best_params['solver'],\n",
        "                                 C=best_params['C'], penalty='l2',\n",
        "                                 random_state=42)\n",
        "    predictions = train_predict_evaluate_model(classifier=logistic, \n",
        "                                               train_features=tfidf_train_features, \n",
        "                                               train_labels=y_train,\n",
        "                                               test_features=tfidf_test_features, \n",
        "                                               test_labels=y_test)\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    \n",
        "    \n",
        "print('Ligistic Regression Overall with TFIDF')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'solver': 'liblinear', 'penalty': 'l2', 'C': 100}\n",
            "Confusion Matrix : [[6 5]\n",
            " [2 7]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.5833333333333334\n",
            "Recall: 0.7777777777777778\n",
            "F1: 0.6666666666666666\n",
            "Confusion Matrix : [[8 2]\n",
            " [3 7]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.7777777777777778\n",
            "Recall: 0.7\n",
            "F1: 0.7368421052631577\n",
            "Confusion Matrix : [[6 4]\n",
            " [2 8]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.8\n",
            "F1: 0.7272727272727272\n",
            "Confusion Matrix : [[7 4]\n",
            " [2 7]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.6363636363636364\n",
            "Recall: 0.7777777777777778\n",
            "F1: 0.7000000000000001\n",
            "Confusion Matrix : [[ 4  4]\n",
            " [ 2 10]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.7142857142857143\n",
            "Recall: 0.8333333333333334\n",
            "F1: 0.7692307692307692\n",
            "Ligistic Regression Overall with TFIDF\n",
            "Acc= 0.7 , std= 0.031622776601683784\n",
            "f1 score= 0.7200024536866642 , std= 0.0346544719338603\n",
            "Precision= 0.6756854256854257 , std= 0.0664368583352485\n",
            "Recalls= 0.7777777777777778 , std= 0.04388537257362558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ8-cfJsURwX",
        "colab_type": "text"
      },
      "source": [
        "#### Old results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8vw6twkp15S",
        "colab_type": "code",
        "outputId": "31b4c2bb-c4d6-4134-b4fa-e73735d66138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "\n",
        "best_params = None\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
        "                                   max_features=300)\n",
        "    \n",
        "    tfidf_train_features = tfidf_vectorizer.fit_transform(x_train)\n",
        "    tfidf_test_features = tfidf_vectorizer.transform(x_test)\n",
        "    \n",
        "    logistic = LogisticRegression(random_state=42)\n",
        "    random_search = RandomizedSearchCV(logistic, param_distributions=params, \n",
        "                                       cv=4, random_state=42)\n",
        "    random_search.fit(tfidf_train_features, y_train)\n",
        "    best_params = random_search.best_params_\n",
        "    print(best_params)\n",
        "    \n",
        "    logistic = LogisticRegression(solver=best_params['solver'],\n",
        "                                 C=best_params['C'], penalty='l2',\n",
        "                                 random_state=42)\n",
        "    predictions = train_predict_evaluate_model(classifier=logistic, \n",
        "                                               train_features=tfidf_train_features, \n",
        "                                               train_labels=y_train,\n",
        "                                               test_features=tfidf_test_features, \n",
        "                                               test_labels=y_test)\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    \n",
        "    \n",
        "print('Ligistic Regression Overall with TFIDF')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'solver': 'liblinear', 'C': 100}\n",
            "Confusion Matrix : [[6 5]\n",
            " [2 7]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.5833333333333334\n",
            "Recall: 0.7777777777777778\n",
            "F1: 0.6666666666666666\n",
            "{'solver': 'lbfgs', 'C': 10}\n",
            "Confusion Matrix : [[7 3]\n",
            " [3 7]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.7\n",
            "Recall: 0.7\n",
            "F1: 0.7\n",
            "{'solver': 'liblinear', 'C': 10}\n",
            "Confusion Matrix : [[6 4]\n",
            " [2 8]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.8\n",
            "F1: 0.7272727272727272\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'solver': 'lbfgs', 'C': 10}\n",
            "Confusion Matrix : [[7 4]\n",
            " [3 6]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.631578947368421\n",
            "{'solver': 'lbfgs', 'C': 100}\n",
            "Confusion Matrix : [[ 4  4]\n",
            " [ 2 10]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.7142857142857143\n",
            "Recall: 0.8333333333333334\n",
            "F1: 0.7692307692307692\n",
            "Ligistic Regression Overall with TFIDF\n",
            "Acc= 0.6799999999999999 , std= 0.024494897427831747\n",
            "f1 score= 0.6989498221077168 , std= 0.047589364878035056\n",
            "Precision= 0.6528571428571428 , std= 0.05256245610113001\n",
            "Recalls= 0.7555555555555555 , std= 0.062459863655800904\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJMrFaLC5r71",
        "colab_type": "text"
      },
      "source": [
        "## LSTM with TFIDF and Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfxnNUBD5xcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\n",
        "from keras.layers import LSTM\n",
        "import keras\n",
        "\n",
        "callbacks_list = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath='lstm_w2vmodel_best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
        "        monitor='val_acc', save_best_only=True),\n",
        "#     keras.callbacks.EarlyStopping(monitor='val_acc', patience=10)\n",
        "]\n",
        "\n",
        "LSTM_DIM = 512 # total LSTM units\n",
        "\n",
        "class LSTM_No_MC():\n",
        "    def __init__(self, vocab_size=None, embedding_matrix=None):\n",
        "        self._build_LSTM_model(vocab_size, embedding_matrix)\n",
        "\n",
        "    def _build_LSTM_model(self, vocab_size=None, embedding_matrix=None):\n",
        "        self.model = Sequential() \n",
        "        if vocab_size is not None:\n",
        "            self.model.add(Embedding(vocab_size,\n",
        "                                    LSTM_DIM, \n",
        "                                    weights=[embedding_matrix],\n",
        "                                    trainable=True))\n",
        "        self.model.add(LSTM(LSTM_DIM,\n",
        "                       recurrent_dropout=0.2,\n",
        "                       activation='tanh'))\n",
        "                           \n",
        "        self.model.add(Dense(LSTM_DIM, activation='tanh')) \n",
        "        self.model.add(Dropout(0.1))\n",
        "        self.model.add(Dense(1, activation='sigmoid'))\n",
        "        self.model.compile(loss='binary_crossentropy', \n",
        "                            optimizer='adam', \n",
        "                            metrics=['accuracy'])\n",
        "        return self\n",
        "    \n",
        "    def fit(self, train_X, train_y):\n",
        "        self.model.fit(train_X, train_y, \n",
        "                       epochs=15, \n",
        "                       batch_size=10, \n",
        "                       shuffle=True, \n",
        "                       verbose=0,\n",
        "#                        callbacks=callbacks_list,\n",
        "#                        validation_split=0.1\n",
        "                      )\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        predictions = self.model.predict(x_test)\n",
        "        return predictions\n",
        "    \n",
        "    def evaluate(self, x_test, y_test):\n",
        "        predictions = self.predict(x_test)\n",
        "        predictions[predictions >= 0.5] = 1\n",
        "        predictions[predictions < 0.5] = 0\n",
        "    \n",
        "        acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "        return acc, f1, recall, precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hvCnAm6Fsy2",
        "colab_type": "text"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-0Cf0Z8XE3-",
        "colab_type": "code",
        "outputId": "9918446a-9910-409b-a74e-17e2c6703a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "np.random.seed(1)\n",
        "\n",
        "corpus = df['normalized_text'].values\n",
        "labels = df['HS'].values\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "MAX_NB_WORDS = 512\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "EMBEDDING_DIM = 512\n",
        "\n",
        "texts = corpus\n",
        "sentences_as_words = []\n",
        "for sent in texts:\n",
        "    temp = sent.split()\n",
        "    sentences_as_words.append(temp)\n",
        "    \n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "print(sequences)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# split the data into a training set and a validation set\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "w2vmodel = gensim.models.Word2Vec(sentences=sentences_as_words,\n",
        "                                   size=EMBEDDING_DIM, \n",
        "                                   workers=4,\n",
        "                                   min_count=2,\n",
        "                                   window=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n",
            "W0329 02:47:05.782345 139832299702144 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[48, 5, 9, 29, 20], [5, 15, 157, 76, 77, 158, 159, 78, 3, 36, 48, 5], [25, 11, 160, 16, 161, 3, 162, 163, 164, 49, 79, 25, 165, 3, 166, 167], [168, 30, 8, 169, 3, 1, 4, 50, 8, 80, 3, 36], [11, 6, 170, 10, 13, 37, 171], [172, 173, 174, 175, 176, 177, 51, 178, 10, 81, 82, 179, 6, 6], [31, 180, 181, 6, 8], [52, 3, 36, 8], [2, 6, 3, 38, 83], [49, 53, 3, 182, 183, 184, 185, 186, 187, 188, 13, 84, 189], [39, 1, 85, 10, 190, 86, 191, 1, 8, 17, 54, 31, 55, 192], [5, 21, 8, 193], [5, 21, 194, 8, 29, 20, 17, 195, 3, 196], [197, 10, 87, 198], [15, 56, 56, 10, 199, 3, 200], [40, 88, 201, 11, 202, 86, 21, 13, 57, 17, 54], [1, 203, 2, 17, 54, 10, 10, 29], [1, 6, 58, 5, 204, 205, 206, 59, 207, 208, 209, 60, 210], [5, 211, 212, 38, 213, 214, 215, 61], [10, 216, 217, 38, 10, 89, 90, 218, 1, 17, 62, 79, 219], [8, 76, 220, 221, 91, 222, 62, 6, 223], [1, 6, 224, 13, 225, 226, 1, 6, 58, 38, 58], [22, 91, 227, 63, 228, 1, 229, 10, 29], [6, 230, 231, 92, 232, 233, 93, 10, 234, 235, 5, 236, 32, 237], [238, 239, 8, 94, 2, 5, 3, 41, 95, 39, 1, 59], [96, 240, 241, 242, 243, 1, 92], [52, 244, 245, 96, 5, 1, 42, 97, 246, 247, 248, 8], [64, 95, 11, 249, 250, 1, 25], [251, 252, 253, 254, 16, 2, 98, 41], [31, 1, 255, 6, 256, 257, 2, 258, 93], [259, 260, 62, 6, 99, 100, 261, 262, 1, 263, 264, 265, 266, 267, 101], [1, 40, 268, 269, 270, 102, 271, 272, 273], [274, 275, 78, 3, 36, 276, 39, 1, 277, 278, 279, 280, 41, 281], [282, 65, 30, 43, 283, 32, 284, 285, 56, 286, 103, 287, 8], [288, 289, 290, 291, 20, 66], [292, 25, 104, 293, 294, 105, 104, 295, 3, 106, 296, 31], [1, 16, 297, 107, 17, 298, 102, 299, 66, 107, 300, 301], [302, 303, 108, 304, 1, 21, 305, 109, 5], [306, 307, 63], [5, 308, 21], [110, 309, 105, 16, 310, 13, 11, 111, 30, 17, 22, 20], [8, 1, 311, 5, 50, 5, 50, 67, 112, 312, 313, 67, 3, 32, 113], [314, 12, 315, 316, 110, 97, 317, 318, 319, 6, 320], [321, 21, 322, 12, 323, 101, 90, 324, 325], [326, 2, 98, 41, 114, 327, 328, 16, 329, 330], [68, 331, 115, 68, 99, 100, 115, 68, 6, 5, 39, 1, 48, 57, 116], [29, 332, 25, 333, 5, 334, 335, 336, 337, 338, 109, 52], [339, 117, 118, 20, 13, 61, 113, 67, 117, 118, 340, 25, 341, 21], [20, 119], [342, 343, 344, 345, 346, 13, 347, 116, 348, 349, 13, 120, 1, 20], [350, 351, 121, 122, 69, 18, 352, 353, 7, 70, 123, 37, 2, 4, 124, 354, 355, 356, 1, 37, 125], [357, 126, 15, 126, 358, 127, 359, 9, 18, 26, 128, 360, 361, 9, 362, 14, 15, 129, 363, 14, 129, 125, 2, 27, 4, 42, 127, 14, 364, 365, 366, 31, 120, 367], [23, 24, 19, 33, 28, 7, 2, 4], [44, 4, 368, 369, 370], [130, 1, 44, 371, 372, 373, 374, 375, 376, 377, 22, 131, 378, 379, 380, 132], [2, 27, 59, 28, 381, 133, 382, 3, 32], [383, 134, 384, 385, 386, 387, 14, 388, 389, 390, 391, 135, 392, 134, 393, 394, 123, 44, 27], [23, 24, 19, 33, 28, 7, 2, 4, 45, 46], [395, 396, 397, 94, 398, 399, 400, 401, 402, 47, 23, 403, 71, 26, 404, 405, 406, 2, 4, 30, 23, 71, 1, 16, 407, 47, 408], [43, 136, 137, 409, 410, 411, 7, 412, 413, 14, 18, 414, 415], [416, 7, 2, 4], [18, 138, 417, 418, 34, 6, 419, 420, 34, 421], [422, 71, 423, 424, 425, 426, 427, 57, 428, 429, 430, 63, 139], [431, 26, 9, 14, 17, 3, 2, 27, 4], [1, 19, 132, 432, 66, 433, 1, 22, 18, 26, 9, 434, 140, 2, 4], [141, 87, 435, 436, 437, 12, 438, 439, 12, 440, 55, 441, 141, 442, 114, 443, 444, 445, 1, 9], [446, 447, 3, 448, 449, 11, 111, 450, 72, 451], [23, 24, 19, 33, 28, 7, 2, 4, 45, 46], [142, 452, 453, 454, 455, 456, 122, 73, 7, 9, 143, 81, 73, 9, 142, 457, 458, 459, 2, 4, 460], [74, 461, 1, 35, 7, 462], [103, 1, 18, 22, 131, 463], [18, 138, 64, 77, 464, 465, 466, 467, 468, 22, 144], [11, 469, 22, 145, 18, 26, 470, 471, 472, 88, 2, 4, 473, 11, 124, 74, 474, 475, 476, 2, 477, 478], [479, 1, 480, 16, 75, 481, 72, 482, 483, 1, 13, 3, 146, 128, 484, 1, 40, 485, 486, 140, 75, 49, 12, 75, 14, 487, 488, 489, 147, 490], [491, 492, 493, 53, 7, 35, 494, 1, 6, 495, 496, 497, 498, 83], [499, 70, 148, 500, 501, 502, 503, 69, 504, 505, 506, 149, 507, 508, 70, 69, 509, 510, 47, 112, 149, 15, 7, 35, 511, 1], [47, 32, 150], [], [23, 24, 19, 33, 28, 7, 2, 4, 151, 27, 146, 133, 45, 46], [106, 16, 152, 24, 137, 24, 1, 35, 7, 144, 89, 136, 19], [43], [151], [1, 74, 53, 12, 85, 1, 42, 135], [108, 14, 1, 139, 150, 15, 14, 15, 7, 35], [60, 147], [65, 1], [34, 130, 84, 153], [4, 119, 34, 154, 34, 2, 65, 9], [60, 12, 11, 155, 155, 9, 11], [152, 44, 27], [82], [1, 19, 12, 37, 145, 72], [61, 40], [26, 43, 12], [23, 24, 19, 33, 28, 7, 2, 4, 45, 46], [121, 15, 9, 143, 73, 7, 2, 4], [156, 51, 55, 156, 51, 148, 42, 30], [154, 153, 80], [12, 64], []]\n",
            "Found 670 unique tokens.\n",
            "Shape of data tensor: (100, 500)\n",
            "Shape of label tensor: (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW0XucqbXMhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data\n",
        "y = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNuNRpBwXRyA",
        "colab_type": "code",
        "outputId": "8b813d9d-916a-4298-c3b3-55ffc0b20a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "embedding_matrix = np.zeros((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    try:\n",
        "        embedding_vector = w2vmodel[word]\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i]=embedding_vector\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udnOLWWKXXRz",
        "colab_type": "code",
        "outputId": "1fbb157d-dd04-4389-c104-d2d6b6865a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        }
      },
      "source": [
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "np.random.seed(1)\n",
        "\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    model = LSTM_No_MC(vocab_size=MAX_SEQUENCE_LENGTH,\n",
        "                      embedding_matrix=embedding_matrix)\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    print('*'*10)\n",
        "    print('Evaluation model ', count + 1, '/5')\n",
        "    count += 1\n",
        "\n",
        "    acc, f1, recall, precision = model.evaluate(x_test, y_test)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "\n",
        "print('LSTM Overall with Word2Vec')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**********\n",
            "Evaluation model  1 /5\n",
            "Confusion Matrix : [[9 2]\n",
            " [4 5]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.7142857142857143\n",
            "Recall: 0.5555555555555556\n",
            "F1: 0.6250000000000001\n",
            "**********\n",
            "Evaluation model  2 /5\n",
            "Confusion Matrix : [[7 6]\n",
            " [3 4]]\n",
            "                 \n",
            "Accuracy: 0.55\n",
            "Precision: 0.4\n",
            "Recall: 0.5714285714285714\n",
            "F1: 0.47058823529411764\n",
            "**********\n",
            "Evaluation model  3 /5\n",
            "Confusion Matrix : [[11  3]\n",
            " [ 2  4]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.5714285714285714\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.6153846153846153\n",
            "**********\n",
            "Evaluation model  4 /5\n",
            "Confusion Matrix : [[ 4  2]\n",
            " [ 4 10]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.8333333333333334\n",
            "Recall: 0.7142857142857143\n",
            "F1: 0.7692307692307692\n",
            "**********\n",
            "Evaluation model  5 /5\n",
            "Confusion Matrix : [[4 2]\n",
            " [5 9]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.8181818181818182\n",
            "Recall: 0.6428571428571429\n",
            "F1: 0.7200000000000001\n",
            "LSTM Overall with Word2Vec\n",
            "Acc= 0.67 , std= 0.06782329983125265\n",
            "f1 score= 0.6400407239819005 , std= 0.10259809608787815\n",
            "Precision= 0.6674458874458875 , std= 0.16320573677610725\n",
            "Recalls= 0.6301587301587301 , std= 0.05930648156276001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQUghPUgIGB7",
        "colab_type": "text"
      },
      "source": [
        "### TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je-iPJu7t-un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "np.random.seed(1)\n",
        "corpus = df['normalized_text'].values\n",
        "labels = df['HS'].values\n",
        "\n",
        "X = corpus.copy()\n",
        "y = labels\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
        "                                   max_features=300)\n",
        "X = tfidf_vectorizer.fit_transform(X)\n",
        "X = X.todense()\n",
        "X = np.asarray(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9tnl4obIJb6",
        "colab_type": "code",
        "outputId": "778b5659-56cd-47f0-a0d1-f10e94366736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        }
      },
      "source": [
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    x_train = np.reshape(x_train,\n",
        "                        (x_train.shape[0], 1, \n",
        "                         x_train.shape[1]))\n",
        "    x_test = np.reshape(x_test, \n",
        "                        (x_test.shape[0], 1, \n",
        "                         x_test.shape[1]))\n",
        "\n",
        "    model = LSTM_No_MC()\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    print('*'*10)\n",
        "    print('Evaluation model ', count + 1, '/5')\n",
        "    count += 1\n",
        "\n",
        "    acc, f1, recall, precision = model.evaluate(x_test, y_test)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    \n",
        "print('LSTM Overall with TF-IDF')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**********\n",
            "Evaluation model  1 /5\n",
            "Confusion Matrix : [[5 6]\n",
            " [2 7]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.5384615384615384\n",
            "Recall: 0.7777777777777778\n",
            "F1: 0.6363636363636364\n",
            "**********\n",
            "Evaluation model  2 /5\n",
            "Confusion Matrix : [[9 1]\n",
            " [3 7]]\n",
            "                 \n",
            "Accuracy: 0.8\n",
            "Precision: 0.875\n",
            "Recall: 0.7\n",
            "F1: 0.7777777777777777\n",
            "**********\n",
            "Evaluation model  3 /5\n",
            "Confusion Matrix : [[6 4]\n",
            " [1 9]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.6923076923076923\n",
            "Recall: 0.9\n",
            "F1: 0.7826086956521738\n",
            "**********\n",
            "Evaluation model  4 /5\n",
            "Confusion Matrix : [[9 2]\n",
            " [5 4]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.4444444444444444\n",
            "F1: 0.5333333333333333\n",
            "**********\n",
            "Evaluation model  5 /5\n",
            "Confusion Matrix : [[4 4]\n",
            " [3 9]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6923076923076923\n",
            "Recall: 0.75\n",
            "F1: 0.7199999999999999\n",
            "LSTM Overall with TF-IDF\n",
            "Acc= 0.69 , std= 0.07348469228349536\n",
            "f1 score= 0.6900166886253841 , std= 0.09444851649234885\n",
            "Precision= 0.6929487179487179 , std= 0.10742488163340627\n",
            "Recalls= 0.7144444444444444 , std= 0.15020972992107723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpeOQF-8800p",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxIs33UN_H8N",
        "colab_type": "text"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNfaARhC85w9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\n",
        "from keras.layers import LSTM\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "np.random.seed(1)\n",
        "LSTM_DIM = 512 # total LSTM units\n",
        "\n",
        "callbacks_list = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath='Lstm_MC_w2vmodel_best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
        "        monitor='val_acc', save_best_only=True),\n",
        "#     keras.callbacks.EarlyStopping(monitor='val_acc', patience=10)\n",
        "]\n",
        "class LSTM_MC():\n",
        "    def __init__(self, vocab_size=None, embedding_matrix=None):\n",
        "        self._build_LSTM_model(vocab_size, embedding_matrix)\n",
        "\n",
        "    def _build_LSTM_model(self, vocab_size=None, embedding_matrix=None):\n",
        "        self.model = Sequential()\n",
        "        if vocab_size is not None:\n",
        "            self.model.add(Embedding(vocab_size,\n",
        "                                    LSTM_DIM, weights=[embedding_matrix],\n",
        "                                    trainable=True))\n",
        "        self.model.add(LSTM(LSTM_DIM,\n",
        "                            recurrent_dropout=0.2,\n",
        "                            kernel_regularizer=l2(1e-4),\n",
        "                            bias_regularizer=l2(1e-4),\n",
        "                            dropout=0.5,\n",
        "                           activation='tanh'))\n",
        "        self.model.add(Dense(LSTM_DIM, activation='tanh',\n",
        "                            kernel_regularizer=l2(1e-4),\n",
        "                            bias_regularizer=l2(1e-4)\n",
        "                            ))\n",
        "        self.model.add(Dropout(0.5))\n",
        "        self.model.add(Dense(1, kernel_regularizer=l2(1e-4),\n",
        "                             activation='sigmoid'))\n",
        "        self.model.compile(loss='binary_crossentropy', \n",
        "                            optimizer='adam', \n",
        "                            metrics=['accuracy'])\n",
        "        return self\n",
        "    \n",
        "    def fit(self, train_X, train_y):\n",
        "        self.model.fit(train_X, train_y, \n",
        "                       epochs=25, \n",
        "                       batch_size=10, \n",
        "                       shuffle=True, \n",
        "                       verbose=0,\n",
        "                      )\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        predictions = self.model.predict(x_test)\n",
        "        return predictions\n",
        "    \n",
        "    def evaluate(self, test_X, y_test):\n",
        "        T = 100\n",
        "        predict_stochastic = K.function([self.model.layers[0].input,\n",
        "                                        K.learning_phase()],\n",
        "                                        [self.model.layers[-1].output])\n",
        "#         Yt_hat = np.array([predict_stochastic([test_X, 1]) for _ in range(T)])\n",
        "        Yt_hat = np.array([self.model.predict(test_X) for _ in range(T)])\n",
        "        MC_pred = np.mean(Yt_hat, axis=0)\n",
        "        MC_pred = MC_pred.reshape(-1, 1)\n",
        "        MC_pred[MC_pred >= 0.5] = 1\n",
        "        MC_pred[MC_pred < 0.5] = 0\n",
        "    \n",
        "        acc, f1, recall, precision = accuracy_report(y_test, \n",
        "                                                     MC_pred)\n",
        "        return acc, f1, recall, precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctI7fhXazdL6",
        "colab_type": "code",
        "outputId": "a04d93f1-7540-4560-9242-8d9458877f14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "np.random.seed(1)\n",
        "\n",
        "corpus = df['normalized_text'].values\n",
        "labels = df['HS'].values\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "MAX_NB_WORDS = 512\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "EMBEDDING_DIM = 512\n",
        "\n",
        "texts = corpus\n",
        "sentences_as_words = []\n",
        "for sent in texts:\n",
        "    temp = sent.split()\n",
        "    sentences_as_words.append(temp)\n",
        "    \n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# split the data into a training set and a validation set\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "w2vmodel = gensim.models.Word2Vec(sentences=sentences_as_words,\n",
        "                                  size=EMBEDDING_DIM, \n",
        "                                  workers=4,\n",
        "                                  min_count=2,\n",
        "                                  window=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n",
            "W0329 04:53:14.105977 139832299702144 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 670 unique tokens.\n",
            "Shape of data tensor: (100, 512)\n",
            "Shape of label tensor: (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8KtiEuGUi1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data\n",
        "y = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyb-bZQe-jY-",
        "colab_type": "code",
        "outputId": "1ea8cdf7-7a40-4376-f843-02ce045243e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "embedding_matrix = np.zeros((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    try:\n",
        "        embedding_vector = w2vmodel[word]\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i]=embedding_vector\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7Mm3gTx6LMD",
        "colab_type": "code",
        "outputId": "e67affb2-5677-4cdd-d225-256267227293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "print(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [ 9.87375970e-04 -9.85599589e-04  7.90039427e-04 ... -6.06902060e-04\n",
            "   1.53743866e-04 -4.28821921e-04]\n",
            " [-1.58300652e-04 -1.46128450e-05  4.51093976e-04 ...  1.08289336e-04\n",
            "  -4.40097443e-04 -6.23544503e-04]\n",
            " ...\n",
            " [-6.16504520e-04 -4.46031481e-04 -3.23501736e-05 ...  6.12647971e-04\n",
            "  -2.90823606e-04  7.96247507e-04]\n",
            " [-6.16504520e-04 -4.46031481e-04 -3.23501736e-05 ...  6.12647971e-04\n",
            "  -2.90823606e-04  7.96247507e-04]\n",
            " [-6.16504520e-04 -4.46031481e-04 -3.23501736e-05 ...  6.12647971e-04\n",
            "  -2.90823606e-04  7.96247507e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXKsE-5db4cT",
        "colab_type": "code",
        "outputId": "b0dda948-a241-441e-ea76-01860e3c80e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    model = LSTM_MC(vocab_size=MAX_SEQUENCE_LENGTH, \n",
        "                    embedding_matrix=embedding_matrix)\n",
        "#     print(model.model.summary())\n",
        "    model.fit(x_train, y_train)\n",
        "    \n",
        "    print('*'*10)\n",
        "    print('Evaluation model ', count + 1, '/5')\n",
        "    count += 1\n",
        "\n",
        "    acc, f1, recall, precision = model.evaluate(x_test, y_test)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "\n",
        "print('LSTM MC Dropout Overall with Word2Vec')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**********\n",
            "Evaluation model  1 /5\n",
            "Confusion Matrix : [[8 3]\n",
            " [3 6]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.6666666666666666\n",
            "**********\n",
            "Evaluation model  2 /5\n",
            "Confusion Matrix : [[7 6]\n",
            " [2 5]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.45454545454545453\n",
            "Recall: 0.7142857142857143\n",
            "F1: 0.5555555555555556\n",
            "**********\n",
            "Evaluation model  3 /5\n",
            "Confusion Matrix : [[11  3]\n",
            " [ 1  5]]\n",
            "                 \n",
            "Accuracy: 0.8\n",
            "Precision: 0.625\n",
            "Recall: 0.8333333333333334\n",
            "F1: 0.7142857142857143\n",
            "**********\n",
            "Evaluation model  4 /5\n",
            "Confusion Matrix : [[ 4  2]\n",
            " [ 4 10]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.8333333333333334\n",
            "Recall: 0.7142857142857143\n",
            "F1: 0.7692307692307692\n",
            "**********\n",
            "Evaluation model  5 /5\n",
            "Confusion Matrix : [[4 2]\n",
            " [5 9]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.8181818181818182\n",
            "Recall: 0.6428571428571429\n",
            "F1: 0.7200000000000001\n",
            "LSTM MC Dropout Overall with Word2Vec\n",
            "Acc= 0.69 , std= 0.06633249580710801\n",
            "f1 score= 0.6851477411477412 , std= 0.07248357292220062\n",
            "Precision= 0.6795454545454546 , std= 0.1390228921963476\n",
            "Recalls= 0.7142857142857143 , std= 0.06563832739090582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJLUCjFg_d29",
        "colab_type": "text"
      },
      "source": [
        "### TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ypIIxpuosNlv",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\n",
        "from keras.layers import LSTM\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "np.random.seed(1)\n",
        "LSTM_DIM = 512 # total LSTM units\n",
        "\n",
        "callbacks_list = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath='Lstm_MC_w2vmodel_best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
        "        monitor='val_acc', save_best_only=True),\n",
        "#     keras.callbacks.EarlyStopping(monitor='val_acc', patience=10)\n",
        "]\n",
        "class LSTM_MC():\n",
        "    def __init__(self, vocab_size=None, embedding_matrix=None):\n",
        "        self._build_LSTM_model(vocab_size, embedding_matrix)\n",
        "\n",
        "    def _build_LSTM_model(self, vocab_size=None, embedding_matrix=None):\n",
        "        self.model = Sequential()\n",
        "        if vocab_size is not None:\n",
        "            self.model.add(Embedding(vocab_size,\n",
        "                                    LSTM_DIM, weights=[embedding_matrix],\n",
        "                                    trainable=True))\n",
        "        self.model.add(LSTM(LSTM_DIM,\n",
        "                            recurrent_dropout=0.2,\n",
        "                            kernel_regularizer=l2(1e-4),\n",
        "                            bias_regularizer=l2(1e-4),\n",
        "                            dropout=0.5,\n",
        "                           activation='tanh'))\n",
        "        self.model.add(Dense(LSTM_DIM, activation='tanh',\n",
        "                            kernel_regularizer=l2(1e-4),\n",
        "                            bias_regularizer=l2(1e-4)\n",
        "                            ))\n",
        "        self.model.add(Dropout(rate=0.5))\n",
        "        self.model.add(Dense(1, activation='sigmoid'))\n",
        "        self.model.compile(loss='binary_crossentropy', \n",
        "                            optimizer='adam', \n",
        "                            metrics=['accuracy'])\n",
        "        return self\n",
        "    \n",
        "    def fit(self, train_X, train_y):\n",
        "        self.model.fit(train_X, train_y, \n",
        "                       epochs=20, \n",
        "                       batch_size=10, \n",
        "                       shuffle=True, \n",
        "                       verbose=0,\n",
        "#                        callbacks=callbacks_list\n",
        "                      )\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        predictions = self.model.predict(x_test)\n",
        "        return predictions\n",
        "    \n",
        "    def evaluate(self, test_X, y_test):\n",
        "        T = 100\n",
        "        predict_stochastic = K.function([self.model.layers[0].input,\n",
        "                                        K.learning_phase()],\n",
        "                                        [self.model.layers[-1].output])\n",
        "#         Yt_hat = np.array([predict_stochastic([test_X, 1]) for _ in range(T)])\n",
        "        Yt_hat = np.array([self.model.predict(test_X) for _ in range(T)])\n",
        "        MC_pred = np.mean(Yt_hat, axis=0)\n",
        "        MC_pred = MC_pred.reshape(-1, 1)\n",
        "        MC_pred[MC_pred >= 0.5] = 1\n",
        "        MC_pred[MC_pred < 0.5] = 0\n",
        "    \n",
        "        acc, f1, recall, precision = accuracy_report(y_test, \n",
        "                                                     MC_pred)\n",
        "        return acc, f1, recall, precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3Lnl4VF_f_r",
        "colab_type": "code",
        "outputId": "ff53b123-4390-455f-db6d-37da9561b45e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "np.random.seed(1)\n",
        "\n",
        "corpus = df['normalized_text'].values\n",
        "labels = df['HS'].values\n",
        "\n",
        "X = corpus.copy()\n",
        "y = labels\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
        "                                   max_features=300)\n",
        "\n",
        "X = tfidf_vectorizer.fit_transform(X)\n",
        "X = X.todense()\n",
        "X = np.asarray(X)\n",
        "\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    x_train = np.reshape(x_train,\n",
        "                        (x_train.shape[0], 1, \n",
        "                         x_train.shape[1]))\n",
        "\n",
        "    x_test = np.reshape(x_test, (x_test.shape[0], 1, \n",
        "                                 x_test.shape[1]))\n",
        "\n",
        "    model = LSTM_MC()\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # Prediction on the test set.\n",
        "    print('*'*10)\n",
        "    print('Evaluation model ', count + 1, '/5')\n",
        "    count += 1\n",
        "\n",
        "    acc, f1, recall, precision = model.evaluate(x_test, y_test)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "\n",
        "print('LSTM MC Dropout Overall with TF-IDF')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**********\n",
            "Evaluation model  1 /5\n",
            "Confusion Matrix : [[5 6]\n",
            " [2 7]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.5384615384615384\n",
            "Recall: 0.7777777777777778\n",
            "F1: 0.6363636363636364\n",
            "**********\n",
            "Evaluation model  2 /5\n",
            "Confusion Matrix : [[7 3]\n",
            " [1 9]]\n",
            "                 \n",
            "Accuracy: 0.8\n",
            "Precision: 0.75\n",
            "Recall: 0.9\n",
            "F1: 0.8181818181818182\n",
            "**********\n",
            "Evaluation model  3 /5\n",
            "Confusion Matrix : [[5 5]\n",
            " [1 9]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.6428571428571429\n",
            "Recall: 0.9\n",
            "F1: 0.75\n",
            "**********\n",
            "Evaluation model  4 /5\n",
            "Confusion Matrix : [[10  1]\n",
            " [ 4  5]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.8333333333333334\n",
            "Recall: 0.5555555555555556\n",
            "F1: 0.6666666666666667\n",
            "**********\n",
            "Evaluation model  5 /5\n",
            "Confusion Matrix : [[2 6]\n",
            " [4 8]]\n",
            "                 \n",
            "Accuracy: 0.5\n",
            "Precision: 0.5714285714285714\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.6153846153846153\n",
            "LSTM MC Dropout Overall with TF-IDF\n",
            "Acc= 0.6699999999999999 , std= 0.1077032961426901\n",
            "f1 score= 0.6973193473193473 , std= 0.07583069807982645\n",
            "Precision= 0.6672161172161172 , std= 0.1102673622151187\n",
            "Recalls= 0.76 , std= 0.13418248123956025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bRXcsfhGpYd",
        "colab_type": "text"
      },
      "source": [
        "# SVM and Logistic with ELMo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZf7fLtmI3rP",
        "colab_type": "text"
      },
      "source": [
        "## Define ELMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeS1kvU6Gu5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "np.random.seed(1)\n",
        "\n",
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPGBQ002HAkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding_from_elmo(words_to_embed, elmo=elmo):\n",
        "#     embedding_tensor = elmo(words_to_embed) # <-- removed other params\n",
        "    embeddings = []\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        for w in words_to_embed:\n",
        "            try:\n",
        "                embedding = sess.run(elmo(w))\n",
        "                embeddings.append(embedding)\n",
        "            except:\n",
        "                print(w)\n",
        "    sess.close() \n",
        "    return np.array(embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdFCgNloHv_n",
        "colab_type": "code",
        "outputId": "421e8dcc-3889-4ab2-d645-c297f413b0eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "words_to_embed = [[\"dog\", \"cat\"], [\"sloth\"]] \n",
        "embeddings = get_embedding_from_elmo(words_to_embed)\n",
        "print(embeddings)\n",
        "print(embeddings.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[ 0.62332994, -0.69219965,  0.46805114, ...,  0.06314384,\n",
            "        -0.18220183,  0.40788883],\n",
            "       [ 0.92854553, -0.10769853, -0.08331862, ...,  0.17715633,\n",
            "        -0.26392198,  0.6423401 ]], dtype=float32)\n",
            " array([[0.380059  , 0.16009209, 0.5363406 , ..., 0.10853949, 0.37611425,\n",
            "        0.23169182]], dtype=float32)]\n",
            "(2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlDL5kMEI_FW",
        "colab_type": "text"
      },
      "source": [
        "## Cross-Validation for Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI2EfdgsJFT0",
        "colab_type": "code",
        "outputId": "c55d93e6-f8ec-4f86-eb03-b1b874f958a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "corpus = df['normalized_text'].values\n",
        "labels = df['HS'].values\n",
        "\n",
        "dataset = corpus.copy()\n",
        "y = labels\n",
        "\n",
        "embeddings = elmo(\n",
        "    corpus.copy(),\n",
        "    signature=\"default\",\n",
        "    as_dict=True)[\"default\"]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    X = sess.run(embeddings)\n",
        "sess.close()\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0608 11:38:56.306629 140007620867968 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 0.14557736 -0.6152045   0.7554845  ... -0.11874242  0.35724452\n",
            "   0.01485758]\n",
            " [ 0.08743453 -0.43605867  0.23102832 ... -0.07607505  0.06322032\n",
            "   0.04236133]\n",
            " [ 0.29741332 -0.3100091   0.10979225 ...  0.1368657   0.22820032\n",
            "   0.3504001 ]\n",
            " ...\n",
            " [-0.25952473 -0.07544519  0.09260989 ...  0.23549713  0.4060844\n",
            "  -0.0263662 ]\n",
            " [ 0.32088628  0.11954337  0.16007017 ...  0.0176918  -0.23178959\n",
            "   0.1458702 ]\n",
            " [-0.4608928  -0.7838666   0.7523269  ...  0.19918656  0.07722344\n",
            "  -0.14978744]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h9W6QYvs-M11",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "np.random.seed(1)\n",
        "params = {\n",
        "        'C': [0.05, 0.1, 1., 10, 50, 100],\n",
        "        'solver': ['lbfgs', 'liblinear'],\n",
        "#         'penalty': ['l2']\n",
        "}\n",
        "y = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlRPcHrYUbHJ",
        "colab_type": "text"
      },
      "source": [
        "#### New results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRxG4II-RiwP",
        "colab_type": "code",
        "outputId": "bde66d39-ad18-4091-b902-49cbf1838764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "X_data = X.copy()\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=42, \n",
        "           shuffle=True)\n",
        "\n",
        "best_params = None\n",
        "for train_index, test_index in kf.split(X_data, y):\n",
        "    x_train, x_test = X_data[train_index], X_data[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    if best_params is None:\n",
        "        logistic = LogisticRegression(random_state=42,\n",
        "                                      max_iter=200)\n",
        "        random_search = RandomizedSearchCV(logistic, \n",
        "                                           param_distributions=params, \n",
        "                                           cv=4, \n",
        "                                           random_state=42)\n",
        "        random_search.fit(x_train, \n",
        "                          y_train)\n",
        "        best_params = random_search.best_params_\n",
        "        print(best_params)\n",
        "        \n",
        "    logistic = LogisticRegression(solver=best_params['solver'],\n",
        "                                 C=best_params['C'], \n",
        "#                                  penalty=params['penalty'],\n",
        "                                 random_state=42, \n",
        "                                 max_iter=200)\n",
        "    predictions = train_predict_evaluate_model(classifier=logistic, \n",
        "                                               train_features=x_train,\n",
        "                                               train_labels=y_train,\n",
        "                                               test_features=x_test,\n",
        "                                               test_labels=y_test)\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    \n",
        "print('Ligistic Regression Overall with ELMo')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'solver': 'lbfgs', 'C': 50}\n",
            "Confusion Matrix : [[8 3]\n",
            " [2 7]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.7\n",
            "Recall: 0.7777777777777778\n",
            "F1: 0.7368421052631577\n",
            "Confusion Matrix : [[7 3]\n",
            " [4 6]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.6\n",
            "F1: 0.631578947368421\n",
            "Confusion Matrix : [[5 5]\n",
            " [2 8]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6153846153846154\n",
            "Recall: 0.8\n",
            "F1: 0.6956521739130435\n",
            "Confusion Matrix : [[4 7]\n",
            " [3 6]]\n",
            "                 \n",
            "Accuracy: 0.5\n",
            "Precision: 0.46153846153846156\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.5454545454545455\n",
            "Confusion Matrix : [[5 3]\n",
            " [3 9]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.75\n",
            "Recall: 0.75\n",
            "F1: 0.75\n",
            "Ligistic Regression Overall with ELMo\n",
            "Acc= 0.65 , std= 0.08366600265340755\n",
            "f1 score= 0.6719055543998336 , std= 0.07549237432731189\n",
            "Precision= 0.6387179487179487 , std= 0.0988507265562419\n",
            "Recalls= 0.7188888888888889 , std= 0.0746679894062731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN-Kd86AUeBz",
        "colab_type": "text"
      },
      "source": [
        "#### Old results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j44E1fR6uaoL",
        "colab_type": "code",
        "outputId": "2843bd63-37a8-4b00-b1fc-45ff98e31d85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1032
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "X_data = X.copy()\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "best_results = None\n",
        "for train_index, test_index in kf.split(X_data, y):\n",
        "    x_train, x_test = X_data[train_index], X_data[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    logistic = LogisticRegression(random_state=42, max_iter=200)\n",
        "    random_search = RandomizedSearchCV(logistic, param_distributions=params, \n",
        "                                       cv=4, random_state=42)\n",
        "    random_search.fit(x_train, y_train)\n",
        "    best_params = random_search.best_params_\n",
        "        \n",
        "    logistic = LogisticRegression(solver=best_params['solver'],\n",
        "                                 C=best_params['C'], penalty='l2',\n",
        "                                 random_state=42, max_iter=200)\n",
        "    predictions = train_predict_evaluate_model(classifier=logistic, \n",
        "                                               train_features=x_train,\n",
        "                                               train_labels=y_train,\n",
        "                                               test_features=x_test,\n",
        "                                               test_labels=y_test)\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    \n",
        "print('Ligistic Regression Overall with ELMo')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix : [[8 3]\n",
            " [4 5]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.625\n",
            "Recall: 0.5555555555555556\n",
            "F1: 0.5882352941176471\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix : [[7 3]\n",
            " [3 7]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.7\n",
            "Recall: 0.7\n",
            "F1: 0.7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix : [[5 5]\n",
            " [2 8]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6153846153846154\n",
            "Recall: 0.8\n",
            "F1: 0.6956521739130435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix : [[4 7]\n",
            " [3 6]]\n",
            "                 \n",
            "Accuracy: 0.5\n",
            "Precision: 0.46153846153846156\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.5454545454545455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix : [[5 3]\n",
            " [5 7]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.7\n",
            "Recall: 0.5833333333333334\n",
            "F1: 0.6363636363636365\n",
            "Ligistic Regression Overall with ELMo\n",
            "Acc= 0.62 , std= 0.06782329983125268\n",
            "f1 score= 0.6331411299697745 , std= 0.060155789183716925\n",
            "Precision= 0.6203846153846154 , std= 0.08712705814128596\n",
            "Recalls= 0.6611111111111111 , std= 0.08720629720154925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEBQF4iINtng",
        "colab_type": "text"
      },
      "source": [
        "## Cross-Validation for SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bv-YReEH_bzI",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "np.random.seed(1)\n",
        "params = {\n",
        "        'C': [0.01, 0.1, 1., 10, 50, 100],\n",
        "        'kernel': ['rbf'],\n",
        "        'gamma': [0.01, 0.1, 1., 10., 100.]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2qF8v75Uj8F",
        "colab_type": "text"
      },
      "source": [
        "#### New results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNc_u5QjRrKv",
        "colab_type": "code",
        "outputId": "d5133c2c-cfef-4607-c1ce-6fb702ae59f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "X_data = X.copy()\n",
        "\n",
        "best_params = None\n",
        "\n",
        "for train_index, test_index in kf.split(X_data, y):\n",
        "    x_train, x_test = X_data[train_index], X_data[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    if best_params is None:\n",
        "        svm = SVC(random_state=42)\n",
        "        random_search = RandomizedSearchCV(svm, param_distributions=params, \n",
        "                                           cv=4, random_state=42)\n",
        "        random_search.fit(x_train, y_train)\n",
        "        best_params = random_search.best_params_\n",
        "        \n",
        "    svm = SVC(kernel=best_params['kernel'], \n",
        "              C=best_params['C'],\n",
        "              gamma=best_params['gamma'],\n",
        "              random_state=42)\n",
        "    predictions = train_predict_evaluate_model(classifier=svm, \n",
        "                                               train_features=x_train,\n",
        "                                               train_labels=y_train,\n",
        "                                               test_features=x_test,\n",
        "                                               test_labels=y_test)\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    \n",
        "    \n",
        "print('SVM Overall with ELMo')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix : [[8 3]\n",
            " [1 8]]\n",
            "                 \n",
            "Accuracy: 0.8\n",
            "Precision: 0.7272727272727273\n",
            "Recall: 0.8888888888888888\n",
            "F1: 0.7999999999999999\n",
            "Confusion Matrix : [[7 3]\n",
            " [4 6]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.6\n",
            "F1: 0.631578947368421\n",
            "Confusion Matrix : [[4 6]\n",
            " [2 8]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.5714285714285714\n",
            "Recall: 0.8\n",
            "F1: 0.6666666666666666\n",
            "Confusion Matrix : [[4 7]\n",
            " [4 5]]\n",
            "                 \n",
            "Accuracy: 0.45\n",
            "Precision: 0.4166666666666667\n",
            "Recall: 0.5555555555555556\n",
            "F1: 0.4761904761904762\n",
            "Confusion Matrix : [[4 4]\n",
            " [4 8]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.6666666666666666\n",
            "SVM Overall with ELMo\n",
            "Acc= 0.6200000000000001 , std= 0.11224972160321826\n",
            "f1 score= 0.648220551378446 , std= 0.10352662374048581\n",
            "Precision= 0.6097402597402597 , std= 0.10866490725069582\n",
            "Recalls= 0.7022222222222221 , std= 0.12460307350112165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR3IBa-iUoz1",
        "colab_type": "text"
      },
      "source": [
        "#### Old results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va3JG25UOVkN",
        "colab_type": "code",
        "outputId": "76f1286f-ddee-408c-b08c-933f9fed2a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "X_data = X.copy()\n",
        "for train_index, test_index in kf.split(X_data, y):\n",
        "    x_train, x_test = X_data[train_index], X_data[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    svm = SVC(random_state=42)\n",
        "    random_search = RandomizedSearchCV(svm, param_distributions=params, \n",
        "                                       cv=4, random_state=42)\n",
        "    random_search.fit(x_train, y_train)\n",
        "    best_params = random_search.best_params_\n",
        "    print(best_params)\n",
        "    svm = SVC(kernel=best_params['kernel'], \n",
        "              C=best_params['C'],\n",
        "              gamma=best_params['gamma'],\n",
        "              random_state=42)\n",
        "    predictions = train_predict_evaluate_model(classifier=svm, \n",
        "                                               train_features=x_train,\n",
        "                                               train_labels=y_train,\n",
        "                                               test_features=x_test,\n",
        "                                               test_labels=y_test)\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    \n",
        "    \n",
        "print('SVM Overall with ELMo')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'kernel': 'rbf', 'gamma': 1.0, 'C': 10}\n",
            "Confusion Matrix : [[ 0 11]\n",
            " [ 0  9]]\n",
            "                 \n",
            "Accuracy: 0.45\n",
            "Precision: 0.45\n",
            "Recall: 1.0\n",
            "F1: 0.6206896551724138\n",
            "{'kernel': 'rbf', 'gamma': 0.01, 'C': 5}\n",
            "Confusion Matrix : [[7 3]\n",
            " [3 7]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.7\n",
            "Recall: 0.7\n",
            "F1: 0.7\n",
            "{'kernel': 'rbf', 'gamma': 0.01, 'C': 200}\n",
            "Confusion Matrix : [[5 5]\n",
            " [2 8]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6153846153846154\n",
            "Recall: 0.8\n",
            "F1: 0.6956521739130435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'kernel': 'rbf', 'gamma': 0.01, 'C': 200}\n",
            "Confusion Matrix : [[5 6]\n",
            " [4 5]]\n",
            "                 \n",
            "Accuracy: 0.5\n",
            "Precision: 0.45454545454545453\n",
            "Recall: 0.5555555555555556\n",
            "F1: 0.5\n",
            "{'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
            "Confusion Matrix : [[ 5  3]\n",
            " [ 1 11]]\n",
            "                 \n",
            "Accuracy: 0.8\n",
            "Precision: 0.7857142857142857\n",
            "Recall: 0.9166666666666666\n",
            "F1: 0.8461538461538461\n",
            "SVM Overall with ELMo\n",
            "Acc= 0.6199999999999999 , std= 0.12884098726725127\n",
            "f1 score= 0.6724991350478607 , std= 0.11303848306513685\n",
            "Precision= 0.6011288711288711 , std= 0.1329489641570481\n",
            "Recalls= 0.7944444444444444 , std= 0.15697762677732766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg6wcPD0Ib9h",
        "colab_type": "text"
      },
      "source": [
        "# LSTM MC Dropout with ELMo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKiMOb1to83O",
        "colab_type": "text"
      },
      "source": [
        "## Define ELMo Embedding model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI6y63uGtTFG",
        "colab_type": "code",
        "outputId": "a8091691-24c2-46db-8f1e-95344aab8129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import keras\n",
        "import logging\n",
        "# Import our dependencies\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "from keras import backend as K\n",
        "import keras.layers as layers\n",
        "from keras.models import Model, load_model\n",
        "from keras.engine import Layer\n",
        "import numpy as np\n",
        "from keras.layers import LSTM, Dropout\n",
        "from keras.layers import Embedding, Dense, TimeDistributed\n",
        "from keras.layers import Lambda\n",
        "from keras.layers.merge import add\n",
        "from keras.regularizers import l2\n",
        "np.random.seed(1)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0329 10:56:16.215198 140222058874752 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7-ZBwbsIlAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ELMoEmbedding(x):\n",
        "    elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", \n",
        "                            trainable=True)\n",
        "    return elmo_model(tf.squeeze(tf.cast(x, tf.string)), \n",
        "                      signature='default',\n",
        "                      as_dict=True)[\"elmo\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoB3BcRUbeIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ElmoEmbeddingLayer(keras.engine.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.dimensions = 1024\n",
        "        self.trainable = True\n",
        "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', \n",
        "                               trainable=self.trainable, \n",
        "                               name=\"{}_module\".format(self.name))\n",
        "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
        "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
        "    \n",
        "    def call(self, x, mask=None):\n",
        "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
        "                      as_dict=True,\n",
        "                      signature='default',\n",
        "                      )['elmo']\n",
        "        return result\n",
        "    \n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return K.not_equal(inputs, '--PAD--')\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.dimensions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7531N0StBGs",
        "colab_type": "text"
      },
      "source": [
        "## Define LSTM with MC Dropout "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_DYZIUsIrO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_LSTM_MC():\n",
        "    input_text = layers.Input(shape=(1,), dtype='string')\n",
        "    embedding = layers.Lambda(ELMoEmbedding, output_shape=(None, 1024))(input_text) \n",
        "    x = LSTM(units=1024, \n",
        "             kernel_regularizer=l2(1e-4),\n",
        "             bias_regularizer=l2(1e-4),\n",
        "             return_sequences=False,\n",
        "             recurrent_dropout=0.5, \n",
        "             dropout=0.2,\n",
        "             activation='tanh')(embedding)\n",
        "    dense = Dense(1024, \n",
        "                  kernel_regularizer=l2(1e-4),\n",
        "                  bias_regularizer=l2(1e-4),\n",
        "                  activation='tanh')(x)\n",
        "    dropout = Dropout(0.5)(dense)\n",
        "    out = Dense(1, activation=\"sigmoid\")(dropout)\n",
        "    model = Model(input_text, out)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics= ['acc'])\n",
        "    return model\n",
        "\n",
        "    \n",
        "def lstm_fit(model, X_train_pad,y_train):\n",
        "    model.fit(X_train_pad,\n",
        "              y_train,\n",
        "              shuffle=True,\n",
        "              epochs=25,\n",
        "              batch_size=4,\n",
        "              verbose=0\n",
        "             )\n",
        "    return model\n",
        "\n",
        "def lstm_predict(model, X_test_pad, y_test):\n",
        "    T = 500\n",
        "    predict_stochastic = K.function([model.layers[0].input,\n",
        "                                    K.learning_phase()],\n",
        "                                    [model.layers[-1].output])\n",
        "    Yt_hat = np.array([predict_stochastic([X_test_pad, 1]) for _ in range(T)])\n",
        "    MC_pred = np.mean(Yt_hat, axis=0)\n",
        "\n",
        "    MC_pred = MC_pred.reshape(-1, 1)\n",
        "\n",
        "    MC_pred[MC_pred >= 0.5] = 1\n",
        "    MC_pred[MC_pred < 0.5] = 0\n",
        "    return accuracy_report(y_test, MC_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUJMpNMBpHTy",
        "colab_type": "text"
      },
      "source": [
        "## Cross validation for LSTM with MC Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCkKCoz9UZ6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = df['normalized_text'].values\n",
        "labels = df['HS'].values\n",
        "\n",
        "X = corpus.copy()\n",
        "y = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sFmEg5Td8HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1)\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    train_text = [' '.join(t.split()[0:]) for t in x_train.tolist()]\n",
        "    train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "    test_text = [' '.join(t.split()[0:]) for t in x_test.tolist()]\n",
        "    test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "\n",
        "    model = build_LSTM_MC()\n",
        "    print(model.summary())\n",
        "    lstm_fit(model, train_text, y_train)\n",
        "    print('eps ', count + 1, ' fitting') \n",
        "    acc, f1, recall, precision = lstm_predict(model, test_text, y_test)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    count += 1\n",
        "    \n",
        "    \n",
        "print('FFNN Overall with ELMo')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpxEcvEWriep",
        "colab_type": "text"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfUhwpals64g",
        "colab_type": "text"
      },
      "source": [
        "## Define LSTM model with ELMo Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9qDeE5zrndB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "from keras import backend as K\n",
        "import keras.layers as layers\n",
        "from keras.models import Model, load_model\n",
        "from keras.engine import Layer\n",
        "import numpy as np\n",
        "# from keras.layers import LSTM, Dropout\n",
        "from keras.layers import LSTM, Embedding, Dense, Dropout, Lambda\n",
        "from keras.layers.merge import add\n",
        "from keras.regularizers import l2\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "\n",
        "class LSTM_Model:\n",
        "    def __init__(self):\n",
        "        self.model = self._build()\n",
        "        \n",
        "    def _build(self):\n",
        "        input_text = layers.Input(shape=(1,), dtype='string')\n",
        "        embedding = layers.Lambda(ELMoEmbedding, \n",
        "                                  output_shape=(None, 1024))(input_text) \n",
        "        x = LSTM(units=256, \n",
        "                 activation='tanh',\n",
        "                 recurrent_dropout=0.1)(embedding)\n",
        "        dense = Dense(256, activation='tanh')(x)\n",
        "        dense = Dropout(0.1)(dense)\n",
        "        out = Dense(1, activation=\"sigmoid\")(dense)\n",
        "        model = Model(input_text, out)\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics= ['acc'])\n",
        "        return model\n",
        "\n",
        "    def fit(self, X_train_pad,y_train):\n",
        "        self.model.fit(X_train_pad,\n",
        "                       y_train,\n",
        "                       shuffle=True,\n",
        "                       epochs=40,\n",
        "                       batch_size=10,\n",
        "                       verbose=0\n",
        "                     )\n",
        "\n",
        "    def predict(self, X_test_pad, y_test):\n",
        "        predictions = self.model.predict(X_test_pad)\n",
        "        predictions = predictions.reshape(-1, 1)\n",
        "        predictions[predictions >= 0.5] = 1\n",
        "        predictions[predictions < 0.5] = 0\n",
        "        return accuracy_report(y_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuRGVEFntq5x",
        "colab_type": "text"
      },
      "source": [
        "## Cross validation for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1ZvCOnMtz6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = df['normalized_text'].values\n",
        "labels = df['HS'].values\n",
        "\n",
        "X = corpus.copy()\n",
        "y = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVBBsjTYt7RX",
        "colab_type": "code",
        "outputId": "99b22ad9-e8e7-4cf6-cdcd-c5b9ab97b047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "np.random.seed(1)\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "precis = []\n",
        "recalls = []\n",
        "\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    train_text = [' '.join(t.split()[0:]) for t in x_train.tolist()]\n",
        "    train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "    test_text = [' '.join(t.split()[0:]) for t in x_test.tolist()]\n",
        "    test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "\n",
        "    model = LSTM_Model()\n",
        "    model.fit(train_text, y_train)\n",
        "    print('eps ', count + 1, ' fitting') \n",
        "    acc, f1, recall, precision = model.predict(test_text, y_test)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    precis.append(precision)\n",
        "    recalls.append(recall)\n",
        "    count += 1\n",
        "    \n",
        "    \n",
        "print('LSTM Overall with ELMo')\n",
        "print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Precision=', np.mean(precis), ', std=', np.std(precis))\n",
        "print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eps  1  fitting\n",
            "Confusion Matrix : [[11  0]\n",
            " [ 2  7]]\n",
            "                 \n",
            "Accuracy: 0.9\n",
            "Precision: 1.0\n",
            "Recall: 0.7777777777777778\n",
            "F1: 0.8750000000000001\n",
            "eps  2  fitting\n",
            "Confusion Matrix : [[6 4]\n",
            " [3 7]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6363636363636364\n",
            "Recall: 0.7\n",
            "F1: 0.6666666666666666\n",
            "eps  3  fitting\n",
            "Confusion Matrix : [[4 6]\n",
            " [2 8]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.5714285714285714\n",
            "Recall: 0.8\n",
            "F1: 0.6666666666666666\n",
            "eps  4  fitting\n",
            "Confusion Matrix : [[5 6]\n",
            " [3 6]]\n",
            "                 \n",
            "Accuracy: 0.55\n",
            "Precision: 0.5\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.5714285714285715\n",
            "eps  5  fitting\n",
            "Confusion Matrix : [[6 2]\n",
            " [6 6]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.75\n",
            "Recall: 0.5\n",
            "F1: 0.6\n",
            "LSTM Overall with ELMo\n",
            "Acc= 0.66 , std= 0.12409673645990857\n",
            "f1 score= 0.6759523809523811 , std= 0.10628865843336548\n",
            "Precision= 0.6915584415584416 , std= 0.1747706494106755\n",
            "Recalls= 0.6888888888888889 , std= 0.1063420987911591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yIyZbpXwlFH",
        "colab_type": "text"
      },
      "source": [
        "# Universal Sentences Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TSZFUwMx8Ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "np.random.seed(1)\n",
        "tf.random.set_random_seed(1)\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJPnaF50yBpB",
        "colab_type": "code",
        "outputId": "39234bf0-7dae-4659-f71d-6982ea6c07f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "dataset = pd.read_csv('/content/gdrive/My Drive/cleaned_text_v3.csv')\n",
        "del dataset['Unnamed: 0']\n",
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>HS</th>\n",
              "      <th>TR</th>\n",
              "      <th>AG</th>\n",
              "      <th>normalized_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @vaintshit:  shut the fuck up and come suck...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>rt shut the fuck up and come suck my dick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>@ArianasBotch Ok if you fucking said leave blo...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ok if you fucking said leave block me. but dm ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>@CyV_SW Wow mo cock got hard. Want to pull you...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>wow mo cock got hard. want to pull your pantie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Ill kill the bitch (chloe) when your not home ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>ill kill the bitch (chloe) when your not home ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>...............................'I get to rape ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>.'i get to rape beautiful women and that is wh...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                    normalized_text\n",
              "0   1  ...         rt shut the fuck up and come suck my dick \n",
              "1   2  ...  ok if you fucking said leave block me. but dm ...\n",
              "2   3  ...  wow mo cock got hard. want to pull your pantie...\n",
              "3   4  ...  ill kill the bitch (chloe) when your not home ...\n",
              "4   5  ...  .'i get to rape beautiful women and that is wh...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktbq1MDxwv9w",
        "colab_type": "text"
      },
      "source": [
        "## Define Universal encoder Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S12UcGBWwq1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" \n",
        "# module_url = \"https://tfhub.dev/google/nnlm-en-dim128/1\"\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnC7wh_fxD3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding(module_url, texts):\n",
        "    embed = hub.Module(module_url)\n",
        "    X = texts\n",
        "\n",
        "    with tf.Session() as session:\n",
        "        session.run([tf.global_variables_initializer(), \n",
        "                    tf.tables_initializer()])\n",
        "        message_embeddings = session.run(embed(X))\n",
        "    session.close()\n",
        "    text_output = np.array(message_embeddings)\n",
        "    \n",
        "    return text_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx-tr-kUyJx4",
        "colab_type": "text"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekbQC5-mxSw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\n",
        "from keras.layers import LSTM\n",
        "\n",
        "LSTM_DIM = 1024 # total LSTM units\n",
        "\n",
        "class LSTM_No_MC():\n",
        "    def __init__(self):\n",
        "        self._build_LSTM_model()\n",
        "\n",
        "    def _build_LSTM_model(self):\n",
        "        self.model = Sequential() \n",
        "        self.model.add(LSTM(LSTM_DIM,\n",
        "                       recurrent_dropout=0.1)) \n",
        "        self.model.add(Dense(1024, activation='relu')) \n",
        "        self.model.add(Dense(1, activation='sigmoid'))\n",
        "        self.model.compile(loss='binary_crossentropy', \n",
        "                            optimizer='adam', \n",
        "                            metrics=['accuracy'])\n",
        "        return self\n",
        "    \n",
        "    def fit(self, train_X, train_y):\n",
        "        self.model.fit(train_X, train_y, \n",
        "              epochs=20, \n",
        "              batch_size=4, \n",
        "              shuffle=True, \n",
        "              verbose=0)\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        predictions = self.model.predict(x_test)\n",
        "        return predictions\n",
        "    \n",
        "    def evaluate(self, x_test, y_test):\n",
        "        predictions = self.predict(x_test)\n",
        "        predictions[predictions >= 0.5] = 1\n",
        "        predictions[predictions < 0.5] = 0\n",
        "    \n",
        "        acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "        return acc, f1, recall, precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3pDHKOYyNul",
        "colab_type": "text"
      },
      "source": [
        "### LSTM cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykECy0MVzDpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_cv(X, y):\n",
        "    kf = KFold(n_splits=5, random_state=42, \n",
        "               shuffle=True)\n",
        "    count = 0\n",
        "    mean_acc = []\n",
        "    f1s = []\n",
        "    recalls = []\n",
        "    precis = []\n",
        "    for train_index, test_index in kf.split(X, y):\n",
        "        x_train, x_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        input_length = x_train.shape[1]\n",
        "        x_train = np.reshape(x_train,\n",
        "                                (x_train.shape[0], 1, \n",
        "                                 x_train.shape[1]))\n",
        "\n",
        "        x_test= np.reshape(x_test, (x_test.shape[0], 1, \n",
        "                                    x_test.shape[1]))\n",
        "        print('*'*50)\n",
        "        model = LSTM_No_MC()\n",
        "        model.fit(x_train, y_train)\n",
        "\n",
        "        # Prediction on the test set.\n",
        "        print('*'*10)\n",
        "        print('Evaluation model ', count + 1, '/5')\n",
        "        count += 1\n",
        "        # predictions = model.predict(x_test)\n",
        "\n",
        "        acc, f1, recal, precision = model.evaluate(x_test, y_test)\n",
        "\n",
        "        mean_acc.append(acc)\n",
        "        f1s.append(f1)\n",
        "        recalls.append(recal)\n",
        "        precis.append(precision)\n",
        "\n",
        "    print('Acc = ', np.mean(mean_acc), ', std = ', np.std(mean_acc))\n",
        "    print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))\n",
        "    print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))\n",
        "    print('Precisions=', np.mean(precis), ', std=', np.std(precis))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDonQwEOS1IB",
        "colab_type": "code",
        "outputId": "90f79e8e-3b40-419d-8aad-a689b495968a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "texts = dataset['normalized_text'].values\n",
        "X = get_embedding(module_url, texts)\n",
        "y = dataset['HS'].values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0608 09:19:53.800340 140007620867968 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHvZp0d_xZaK",
        "colab_type": "code",
        "outputId": "b9a708c8-542b-414d-ebd8-31d88796b244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1190
        }
      },
      "source": [
        "lstm_cv(X, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "**********\n",
            "Evaluation model  1 /5\n",
            "Confusion Matrix : [[5 6]\n",
            " [4 5]]\n",
            "Accuracy :  0.5\n",
            "Sensitivity :  0.45454545454545453\n",
            "Specificity :  0.5555555555555556\n",
            "                 \n",
            "Accuracy: 0.5\n",
            "Precision: 0.45454545454545453\n",
            "Recall: 0.5555555555555556\n",
            "F1: 0.5\n",
            "**************************************************\n",
            "**********\n",
            "Evaluation model  2 /5\n",
            "Confusion Matrix : [[5 5]\n",
            " [1 9]]\n",
            "Accuracy :  0.7\n",
            "Sensitivity :  0.5\n",
            "Specificity :  0.9\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.6428571428571429\n",
            "Recall: 0.9\n",
            "F1: 0.75\n",
            "**************************************************\n",
            "**********\n",
            "Evaluation model  3 /5\n",
            "Confusion Matrix : [[6 4]\n",
            " [1 9]]\n",
            "Accuracy :  0.75\n",
            "Sensitivity :  0.6\n",
            "Specificity :  0.9\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.6923076923076923\n",
            "Recall: 0.9\n",
            "F1: 0.7826086956521738\n",
            "**************************************************\n",
            "**********\n",
            "Evaluation model  4 /5\n",
            "Confusion Matrix : [[9 2]\n",
            " [4 5]]\n",
            "Accuracy :  0.7\n",
            "Sensitivity :  0.8181818181818182\n",
            "Specificity :  0.5555555555555556\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.7142857142857143\n",
            "Recall: 0.5555555555555556\n",
            "F1: 0.6250000000000001\n",
            "**************************************************\n",
            "**********\n",
            "Evaluation model  5 /5\n",
            "Confusion Matrix : [[5 3]\n",
            " [4 8]]\n",
            "Accuracy :  0.65\n",
            "Sensitivity :  0.625\n",
            "Specificity :  0.6666666666666666\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.7272727272727273\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.6956521739130435\n",
            "Acc =  0.6599999999999999 , std =  0.08602325267042626\n",
            "F1 =  0.6706521739130434 , std= 0.10065945132861062\n",
            "Recalls =  0.7155555555555556 , std= 0.1559677079588141\n",
            "Precisions= 0.6462537462537462 , std= 0.10007696631207583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16IPpBgixkbN",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcIGFeY2Mscm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "np.random.seed(1)\n",
        "\n",
        "params = {\n",
        "        'C': [0.01, 0.1, 1., 10, 100, 200],\n",
        "        'solver': ['liblinear', 'lbfgs'],\n",
        "        'penalty': ['l2']\n",
        "}\n",
        "kf = KFold(n_splits=5, random_state=42, \n",
        "           shuffle=True)\n",
        "texts = dataset['normalized_text'].values\n",
        "y = dataset['HS'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnkvBNWBUzJo",
        "colab_type": "text"
      },
      "source": [
        "#### New results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsL5ZkEoU2k-",
        "colab_type": "code",
        "outputId": "5408584b-226f-456d-d12b-ef1ef457f33f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        }
      },
      "source": [
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "recalls = []\n",
        "precis = []\n",
        "\n",
        "best_params = None\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    print('*'*50)\n",
        "    if best_params is None:\n",
        "        logistic = LogisticRegression(random_state=42, max_iter=500)\n",
        "        random_search = RandomizedSearchCV(logistic, random_state=42,\n",
        "                                           param_distributions=params,\n",
        "                                           cv=4)\n",
        "        random_search.fit(x_train, y_train)\n",
        "        best_params = random_search.best_params_\n",
        "        print(best_params)\n",
        "    logistic = LogisticRegression(solver=best_params['solver'],\n",
        "                                 C=best_params['C'], penalty='l2',\n",
        "                                 random_state=42)\n",
        "    logistic.fit(x_train, y_train)\n",
        "    \n",
        "    print('Evaluation model ', count + 1, '/5')\n",
        "    count += 1\n",
        "    \n",
        "    print('Evaluation results:')\n",
        "    \n",
        "    prediction = logistic.predict(x_test) # predicting on the validation set\n",
        "    prediction_int = prediction.astype(np.int)\n",
        "     \n",
        "    acc, f1, recall, precision = accuracy_report(y_test, prediction_int)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    recalls.append(recall)\n",
        "    precis.append(precision)\n",
        "\n",
        "print('Acc = ', np.mean(mean_acc), ', std = ', np.std(mean_acc))\n",
        "print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))\n",
        "print('Precisions=', np.mean(precis), ', std=', np.std(precis))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "{'solver': 'liblinear', 'penalty': 'l2', 'C': 200}\n",
            "Evaluation model  1 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[5 6]\n",
            " [5 4]]\n",
            "                 \n",
            "Accuracy: 0.45\n",
            "Precision: 0.4\n",
            "Recall: 0.4444444444444444\n",
            "F1: 0.4210526315789474\n",
            "**************************************************\n",
            "Evaluation model  2 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[9 1]\n",
            " [4 6]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.8571428571428571\n",
            "Recall: 0.6\n",
            "F1: 0.7058823529411764\n",
            "**************************************************\n",
            "Evaluation model  3 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[7 3]\n",
            " [1 9]]\n",
            "                 \n",
            "Accuracy: 0.8\n",
            "Precision: 0.75\n",
            "Recall: 0.9\n",
            "F1: 0.8181818181818182\n",
            "**************************************************\n",
            "Evaluation model  4 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[9 2]\n",
            " [4 5]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.7142857142857143\n",
            "Recall: 0.5555555555555556\n",
            "F1: 0.6250000000000001\n",
            "**************************************************\n",
            "Evaluation model  5 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[3 5]\n",
            " [3 9]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.6428571428571429\n",
            "Recall: 0.75\n",
            "F1: 0.6923076923076924\n",
            "Acc =  0.66 , std =  0.12409673645990857\n",
            "F1 =  0.6524848990019269 , std= 0.13130797847599593\n",
            "Recalls =  0.65 , std= 0.15885392000588017\n",
            "Precisions= 0.6728571428571428 , std= 0.15295724359227625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yubWpCZqU3J0",
        "colab_type": "text"
      },
      "source": [
        "#### Old results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-JFLQVJxnJw",
        "colab_type": "code",
        "outputId": "943aa5b6-5b9b-4320-a5c6-496dbd80d0ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1142
        }
      },
      "source": [
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "recalls = []\n",
        "precis = []\n",
        "\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    print('*'*50)\n",
        "    logistic = LogisticRegression(random_state=42, max_iter=500)\n",
        "    random_search = RandomizedSearchCV(logistic, random_state=42,\n",
        "                                       param_distributions=params,\n",
        "                                       cv=4)\n",
        "    random_search.fit(x_train, y_train)\n",
        "    best_params = random_search.best_params_\n",
        "    print(best_params)\n",
        "    logistic = LogisticRegression(solver=best_params['solver'],\n",
        "                                 C=best_params['C'], penalty='l2',\n",
        "                                 random_state=42)\n",
        "    logistic.fit(x_train, y_train)\n",
        "    \n",
        "    print('Evaluation model ', count + 1, '/5')\n",
        "    count += 1\n",
        "    \n",
        "    print('Evaluation results:')\n",
        "    \n",
        "    prediction = logistic.predict(x_test) # predicting on the validation set\n",
        "    prediction_int = prediction.astype(np.int)\n",
        "     \n",
        "    acc, f1, recall, precision = accuracy_report(y_test, prediction_int)\n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    recalls.append(recall)\n",
        "    precis.append(precision)\n",
        "\n",
        "print('Acc = ', np.mean(mean_acc), ', std = ', np.std(mean_acc))\n",
        "print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))\n",
        "print('Precisions=', np.mean(precis), ', std=', np.std(precis))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'solver': 'liblinear', 'penalty': 'l2', 'C': 200}\n",
            "Evaluation model  1 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[5 6]\n",
            " [5 4]]\n",
            "                 \n",
            "Accuracy: 0.45\n",
            "Precision: 0.4\n",
            "Recall: 0.4444444444444444\n",
            "F1: 0.4210526315789474\n",
            "**************************************************\n",
            "{'solver': 'lbfgs', 'penalty': 'l2', 'C': 10}\n",
            "Evaluation model  2 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[9 1]\n",
            " [4 6]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.8571428571428571\n",
            "Recall: 0.6\n",
            "F1: 0.7058823529411764\n",
            "**************************************************\n",
            "{'solver': 'liblinear', 'penalty': 'l2', 'C': 100}\n",
            "Evaluation model  3 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[7 3]\n",
            " [1 9]]\n",
            "                 \n",
            "Accuracy: 0.8\n",
            "Precision: 0.75\n",
            "Recall: 0.9\n",
            "F1: 0.8181818181818182\n",
            "**************************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'solver': 'lbfgs', 'penalty': 'l2', 'C': 10}\n",
            "Evaluation model  4 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[9 2]\n",
            " [4 5]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.7142857142857143\n",
            "Recall: 0.5555555555555556\n",
            "F1: 0.6250000000000001\n",
            "**************************************************\n",
            "{'solver': 'liblinear', 'penalty': 'l2', 'C': 200}\n",
            "Evaluation model  5 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[3 5]\n",
            " [3 9]]\n",
            "                 \n",
            "Accuracy: 0.6\n",
            "Precision: 0.6428571428571429\n",
            "Recall: 0.75\n",
            "F1: 0.6923076923076924\n",
            "Acc =  0.66 , std =  0.12409673645990857\n",
            "F1 =  0.6524848990019269 , std= 0.13130797847599593\n",
            "Recalls =  0.65 , std= 0.15885392000588017\n",
            "Precisions= 0.6728571428571428 , std= 0.15295724359227625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m86ldO9xrAZ",
        "colab_type": "text"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78M1sSeAOFoS",
        "colab_type": "code",
        "outputId": "3862ca24-765e-4c26-c0cd-1503834a2426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "np.random.seed(1)\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=42, \n",
        "           shuffle=True)\n",
        "texts = dataset['normalized_text'].values\n",
        "X = get_embedding(module_url, texts)\n",
        "y = dataset['HS'].values\n",
        "\n",
        "params = {\n",
        "        'C': [0.01, 0.1, 1., 10, 100, 200],\n",
        "        'kernel': ['rbf'],\n",
        "        'gamma': ['auto', 0.01, 0.1, 1., 10., 100.]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0608 09:20:13.010547 140007620867968 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzTiIt7aVyXN",
        "colab_type": "text"
      },
      "source": [
        "#### New results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtCXXt6LV1yI",
        "colab_type": "code",
        "outputId": "26af1134-ceb7-4e75-8f09-94aeb9ed2204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "recalls = []\n",
        "precis = []\n",
        "\n",
        "best_params = None\n",
        "\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    if best_params is None:\n",
        "        print('*'*50)\n",
        "        svm = SVC()\n",
        "\n",
        "        random_search = RandomizedSearchCV(svm, param_distributions=params, \n",
        "                                           cv=3, random_state=42)\n",
        "        random_search.fit(x_train, y_train)\n",
        "        best_params = random_search.best_params_\n",
        "    \n",
        "    svm = SVC(random_state=42, C=best_params['C'], \n",
        "             kernel=best_params['kernel'],\n",
        "             gamma=best_params['gamma'])\n",
        "    svm.fit(x_train, y_train)\n",
        "    \n",
        "    print('Evaluation model ', count + 1, '/5')\n",
        "    count += 1\n",
        "    \n",
        "    print('Evaluation results:')\n",
        "    \n",
        "    prediction = svm.predict(x_test) # predicting on the validation set\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, prediction)\n",
        "    \n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    recalls.append(recall)\n",
        "    precis.append(precision)\n",
        "\n",
        "print('Acc = ', np.mean(mean_acc), ', std = ', np.std(mean_acc))\n",
        "print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))\n",
        "print('Precisions=', np.mean(precis), ', std=', np.std(precis))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "Evaluation model  1 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[5 6]\n",
            " [5 4]]\n",
            "                 \n",
            "Accuracy: 0.45\n",
            "Precision: 0.4\n",
            "Recall: 0.4444444444444444\n",
            "F1: 0.4210526315789474\n",
            "Evaluation model  2 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[9 1]\n",
            " [4 6]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.8571428571428571\n",
            "Recall: 0.6\n",
            "F1: 0.7058823529411764\n",
            "Evaluation model  3 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[7 3]\n",
            " [1 9]]\n",
            "                 \n",
            "Accuracy: 0.8\n",
            "Precision: 0.75\n",
            "Recall: 0.9\n",
            "F1: 0.8181818181818182\n",
            "Evaluation model  4 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[9 2]\n",
            " [4 5]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.7142857142857143\n",
            "Recall: 0.5555555555555556\n",
            "F1: 0.6250000000000001\n",
            "Evaluation model  5 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[4 4]\n",
            " [3 9]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6923076923076923\n",
            "Recall: 0.75\n",
            "F1: 0.7199999999999999\n",
            "Acc =  0.67 , std =  0.12083045973594572\n",
            "F1 =  0.6580233605403883 , std= 0.1334376163398494\n",
            "Recalls =  0.65 , std= 0.15885392000588017\n",
            "Precisions= 0.6827472527472528 , std= 0.15229500605237895\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtNqsngcV2Jf",
        "colab_type": "text"
      },
      "source": [
        "#### Old results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMS-uj2sxtOD",
        "colab_type": "code",
        "outputId": "92c855f5-a7e8-453b-c685-7c365217cc64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1174
        }
      },
      "source": [
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "recalls = []\n",
        "precis = []\n",
        "\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    print('*'*50)\n",
        "    svm = SVC()\n",
        "    \n",
        "    random_search = RandomizedSearchCV(svm, param_distributions=params, \n",
        "                                       cv=3, random_state=42)\n",
        "    random_search.fit(x_train, y_train)\n",
        "    best_params = random_search.best_params_\n",
        "    \n",
        "    svm = SVC(random_state=42, C=best_params['C'], \n",
        "             kernel=best_params['kernel'],\n",
        "             gamma=best_params['gamma'])\n",
        "    svm.fit(x_train, y_train)\n",
        "    \n",
        "    print('Evaluation model ', count + 1, '/5')\n",
        "    count += 1\n",
        "    \n",
        "    print('Evaluation results:')\n",
        "    \n",
        "    prediction = svm.predict(x_test) # predicting on the validation set\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, prediction)\n",
        "    \n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    recalls.append(recall)\n",
        "    precis.append(precision)\n",
        "\n",
        "print('Acc = ', np.mean(mean_acc), ', std = ', np.std(mean_acc))\n",
        "print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))\n",
        "print('Precisions=', np.mean(precis), ', std=', np.std(precis))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "Evaluation model  1 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[5 6]\n",
            " [5 4]]\n",
            "                 \n",
            "Accuracy: 0.45\n",
            "Precision: 0.4\n",
            "Recall: 0.4444444444444444\n",
            "F1: 0.4210526315789474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "Evaluation model  2 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[9 1]\n",
            " [4 6]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.8571428571428571\n",
            "Recall: 0.6\n",
            "F1: 0.7058823529411764\n",
            "**************************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation model  3 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[7 3]\n",
            " [1 9]]\n",
            "                 \n",
            "Accuracy: 0.8\n",
            "Precision: 0.75\n",
            "Recall: 0.9\n",
            "F1: 0.8181818181818182\n",
            "**************************************************\n",
            "Evaluation model  4 /5\n",
            "Evaluation results:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix : [[9 2]\n",
            " [4 5]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.7142857142857143\n",
            "Recall: 0.5555555555555556\n",
            "F1: 0.6250000000000001\n",
            "**************************************************\n",
            "Evaluation model  5 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[4 4]\n",
            " [3 9]]\n",
            "                 \n",
            "Accuracy: 0.65\n",
            "Precision: 0.6923076923076923\n",
            "Recall: 0.75\n",
            "F1: 0.7199999999999999\n",
            "Acc =  0.67 , std =  0.12083045973594572\n",
            "F1 =  0.6580233605403883 , std= 0.1334376163398494\n",
            "Recalls =  0.65 , std= 0.15885392000588017\n",
            "Precisions= 0.6827472527472528 , std= 0.15229500605237895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbSlPVdTMizQ",
        "colab_type": "text"
      },
      "source": [
        "## LSTM MC Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCfaxVl9MsYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD, RMSprop, Adagrad\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, SpatialDropout1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "np.random.seed(1)\n",
        "\n",
        "class LSTM_MC:\n",
        "    def __init__(self, input_length):\n",
        "        self.model = self.build()\n",
        "        \n",
        "    def build(self):\n",
        "        print('Build model...')\n",
        "        p_W, p_U, p_dense, p_emb = 0.75, 0.75, 0.5, 0.5\n",
        "        weight_decay, batch_size, maxlen = 1e-4, 10, 500\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(1024, input_shape=(1, input_length),\n",
        "                       kernel_regularizer=l2(weight_decay), \n",
        "                       recurrent_regularizer=l2(weight_decay),\n",
        "                       dropout=p_W, \n",
        "                       recurrent_dropout=p_U))\n",
        "        model.add(Dropout(p_dense))\n",
        "        model.add(Dense(1024, \n",
        "                        kernel_regularizer=l2(weight_decay), \n",
        "                        activation='relu'\n",
        "                       ))\n",
        "        model.add(Dropout(p_dense))\n",
        "        model.add(Dense(1, kernel_regularizer=l2(weight_decay), \n",
        "                        activation='sigmoid'\n",
        "                       ))\n",
        "        model.compile(loss='binary_crossentropy', \n",
        "                      optimizer='adam',\n",
        "                      metrics=['acc'])\n",
        "        return model\n",
        "    \n",
        "    def fit(self, train_X, train_y):\n",
        "        input_length = train_X.shape[1]\n",
        "        train_X = np.reshape(train_X,\n",
        "                                (train_X.shape[0], 1, train_X.shape[1]))\n",
        "        self.model.fit(train_X, train_y, \n",
        "              batch_size=4, \n",
        "              shuffle=True,\n",
        "              epochs=50,\n",
        "              verbose=0)\n",
        "\n",
        "    def predict(self, test_X):\n",
        "        input_length = test_X.shape[1]\n",
        "        test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
        "        T = 1000\n",
        "        predict_stochastic = K.function([self.model.layers[0].input,\n",
        "                                        K.learning_phase()],\n",
        "                                        [self.model.layers[-1].output])\n",
        "        Yt_hat = np.array([predict_stochastic([test_X, 1]) for _ in range(T)])\n",
        "        MC_pred = np.mean(Yt_hat, axis=0)\n",
        "        MC_pred = MC_pred.reshape(-1, 1)\n",
        "        MC_pred[MC_pred >= 0.5] = 1\n",
        "        MC_pred[MC_pred < 0.5] = 0\n",
        "        return MC_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB7VbxFCTIQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = dataset['normalized_text'].values\n",
        "X = get_embedding(module_url, texts)\n",
        "y = dataset['HS'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3xym1ANMxf6",
        "colab_type": "code",
        "outputId": "ea90cf8f-d812-4914-c3ca-5f2ed48f56f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1360
        }
      },
      "source": [
        "kf = KFold(n_splits=5, random_state=42, \n",
        "           shuffle=True)\n",
        "\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "recalls = []\n",
        "precis = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    print('*'*50)\n",
        "    input_length = x_train.shape[1]\n",
        "    model = LSTM_MC(input_length)\n",
        "    model.fit(x_train, y_train)\n",
        "    print('*'*10)\n",
        "    print('Evaluation model ', count + 1, '/5')\n",
        "    count += 1\n",
        "    predictions = model.predict(x_test)\n",
        "    print('Evaluation results:')\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "    \n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    recalls.append(recall)\n",
        "    precis.append(precision)\n",
        "    \n",
        "print('Acc = ', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))\n",
        "print('Precision=', np.mean(precis),', std=', np.std(precis))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "Build model...\n",
            "**********\n",
            "Evaluation model  1 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[7 4]\n",
            " [5 4]]\n",
            "Accuracy :  0.55\n",
            "Sensitivity :  0.6363636363636364\n",
            "Specificity :  0.4444444444444444\n",
            "                 \n",
            "Accuracy: 0.55\n",
            "Precision: 0.5\n",
            "Recall: 0.4444444444444444\n",
            "F1: 0.47058823529411764\n",
            "**************************************************\n",
            "Build model...\n",
            "**********\n",
            "Evaluation model  2 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[8 2]\n",
            " [2 8]]\n",
            "Accuracy :  0.8\n",
            "Sensitivity :  0.8\n",
            "Specificity :  0.8\n",
            "                 \n",
            "Accuracy: 0.8\n",
            "Precision: 0.8\n",
            "Recall: 0.8\n",
            "F1: 0.8000000000000002\n",
            "**************************************************\n",
            "Build model...\n",
            "**********\n",
            "Evaluation model  3 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[6 4]\n",
            " [1 9]]\n",
            "Accuracy :  0.75\n",
            "Sensitivity :  0.6\n",
            "Specificity :  0.9\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.6923076923076923\n",
            "Recall: 0.9\n",
            "F1: 0.7826086956521738\n",
            "**************************************************\n",
            "Build model...\n",
            "**********\n",
            "Evaluation model  4 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[9 2]\n",
            " [3 6]]\n",
            "Accuracy :  0.75\n",
            "Sensitivity :  0.8181818181818182\n",
            "Specificity :  0.6666666666666666\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.75\n",
            "Recall: 0.6666666666666666\n",
            "F1: 0.7058823529411765\n",
            "**************************************************\n",
            "Build model...\n",
            "**********\n",
            "Evaluation model  5 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[ 7  1]\n",
            " [ 2 10]]\n",
            "Accuracy :  0.85\n",
            "Sensitivity :  0.875\n",
            "Specificity :  0.8333333333333334\n",
            "                 \n",
            "Accuracy: 0.85\n",
            "Precision: 0.9090909090909091\n",
            "Recall: 0.8333333333333334\n",
            "F1: 0.8695652173913043\n",
            "Acc =  0.74 , std= 0.10198039027185568\n",
            "F1 =  0.7257289002557544 , std= 0.13779044804778937\n",
            "Recalls =  0.7288888888888889 , std= 0.16126046716912656\n",
            "Precision= 0.7302797202797203 , std= 0.135420731471223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rpj5Xl6HmSpU"
      },
      "source": [
        "## LSTM MC Dropout TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Clnyvo0RmSpX",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD, RMSprop, Adagrad\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, SpatialDropout1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "np.random.seed(1)\n",
        "\n",
        "class LSTM_MC:\n",
        "    def __init__(self, input_length):\n",
        "        self.model = self.build()\n",
        "        \n",
        "    def build(self):\n",
        "        print('Build model...')\n",
        "        p_W, p_U, p_dense = 0.85, 0.5, 0.5\n",
        "        weight_decay, batch_size, maxlen = 1e-4, 10, 500\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(1024, input_shape=(1, input_length),\n",
        "                       kernel_regularizer=l2(weight_decay), \n",
        "                       bias_regularizer=l2(weight_decay),\n",
        "                       recurrent_regularizer=l2(weight_decay),\n",
        "                       dropout=p_W, \n",
        "                       recurrent_dropout=p_U,\n",
        "                       activation='tanh'))\n",
        "        model.add(Dense(1024, \n",
        "                        kernel_regularizer=l2(weight_decay), \n",
        "                       bias_regularizer=l2(weight_decay),\n",
        "                        activation='relu'\n",
        "                       ))\n",
        "        model.add(Dropout(p_dense))\n",
        "        model.add(Dense(1, \n",
        "                        kernel_regularizer=l2(weight_decay), \n",
        "                        bias_regularizer=l2(weight_decay),\n",
        "                        activation='sigmoid'\n",
        "                       ))\n",
        "        model.compile(loss='binary_crossentropy', \n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        return model\n",
        "    \n",
        "    def fit(self, train_X, train_y):\n",
        "        input_length = train_X.shape[1]\n",
        "        train_X = np.reshape(train_X,\n",
        "                                (train_X.shape[0], 1, train_X.shape[1]))\n",
        "        self.model.fit(train_X, train_y, \n",
        "              batch_size=2, \n",
        "              shuffle=True,\n",
        "              epochs=35,\n",
        "              verbose=0)\n",
        "\n",
        "    def predict(self, test_X):\n",
        "        input_length = test_X.shape[1]\n",
        "        test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
        "        T = 1000\n",
        "        predict_stochastic = K.function([self.model.layers[0].input,\n",
        "                                        K.learning_phase()],\n",
        "                                        [self.model.layers[-1].output])\n",
        "        Yt_hat = np.array([predict_stochastic([test_X, 1]) for _ in range(T)])\n",
        "        MC_pred = np.mean(Yt_hat, axis=0)\n",
        "        MC_pred = MC_pred.reshape(-1, 1)\n",
        "        MC_pred[MC_pred >= 0.5] = 1\n",
        "        MC_pred[MC_pred < 0.5] = 0\n",
        "        return MC_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nhcq-Hx4mSpf",
        "outputId": "490be0fa-a3c8-4f79-d877-4a2639d54a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "texts = dataset['normalized_text'].values\n",
        "X = get_embedding(module_url, texts)\n",
        "y = dataset['HS'].values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0326 21:48:09.646178 140181877933952 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0326 21:48:13.652585 140181877933952 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a8ee2f22-c8a7-4173-d90c-ca6be388c16a",
        "id": "rAxOCuxgmSpk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1983
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "kf = KFold(n_splits=5, random_state=42, \n",
        "           shuffle=True)\n",
        "\n",
        "count = 0\n",
        "mean_acc = []\n",
        "f1s = []\n",
        "recalls = []\n",
        "precis = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    x_train, x_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    print('*'*50)\n",
        "    input_length = x_train.shape[1]\n",
        "    model = LSTM_MC(input_length)\n",
        "    model.fit(x_train, y_train)\n",
        "    print('Evaluation model ', count + 1, '/5')\n",
        "    count += 1\n",
        "    predictions = model.predict(x_test)\n",
        "    print('Evaluation results:')\n",
        "    acc, f1, recall, precision = accuracy_report(y_test, predictions)\n",
        "    \n",
        "    mean_acc.append(acc)\n",
        "    f1s.append(f1)\n",
        "    recalls.append(recall)\n",
        "    precis.append(precision)\n",
        "    \n",
        "print('Acc = ', np.mean(mean_acc), ', std=', np.std(mean_acc))\n",
        "print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))\n",
        "print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))\n",
        "print('Precision=', np.mean(precis),', std=', np.std(precis))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "Build model...\n",
            "Evaluation model  1 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[6 5]\n",
            " [5 4]]\n",
            "                 \n",
            "Accuracy: 0.5\n",
            "Precision: 0.4444444444444444\n",
            "Recall: 0.4444444444444444\n",
            "F1: 0.4444444444444444\n",
            "**************************************************\n",
            "Build model...\n",
            "Evaluation model  2 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[6 4]\n",
            " [1 9]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.6923076923076923\n",
            "Recall: 0.9\n",
            "F1: 0.7826086956521738\n",
            "**************************************************\n",
            "Build model...\n",
            "Evaluation model  3 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[ 5  5]\n",
            " [ 0 10]]\n",
            "                 \n",
            "Accuracy: 0.75\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 1.0\n",
            "F1: 0.8\n",
            "**************************************************\n",
            "Build model...\n",
            "Evaluation model  4 /5\n",
            "Evaluation results:\n",
            "Confusion Matrix : [[9 2]\n",
            " [4 5]]\n",
            "                 \n",
            "Accuracy: 0.7\n",
            "Precision: 0.7142857142857143\n",
            "Recall: 0.5555555555555556\n",
            "F1: 0.6250000000000001\n",
            "**************************************************\n",
            "Build model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-4c8328a66c07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_MC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluation model '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-cc85f2803163>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_X, train_y)\u001b[0m\n\u001b[1;32m     48\u001b[0m               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m               verbose=0)\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[512,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_73/RMSprop/gradients/lstm_75/strided_slice_1_grad/StridedSliceGrad}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
          ]
        }
      ]
    }
  ]
}